{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.008019,
     "end_time": "2021-01-02T21:38:02.814357",
     "exception": false,
     "start_time": "2021-01-02T21:38:02.806338",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# JS - PLS + Gradient Boosted Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 5.817261,
     "end_time": "2021-01-02T21:38:08.639513",
     "exception": false,
     "start_time": "2021-01-02T21:38:02.822252",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV, PredefinedSplit\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 1.322753,
     "end_time": "2021-01-02T21:40:21.545483",
     "exception": false,
     "start_time": "2021-01-02T21:40:20.22273",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read data as 32 bit floats\n",
    "file = os.path.join(os.pardir, \"input\", \"jane-street-market-prediction\", \"train.csv\")\n",
    "dtype = {c: np.float32 for c in pd.read_csv(file, nrows=1).columns}\n",
    "full_df = pd.read_csv(file, engine=\"c\", dtype=dtype)\n",
    "\n",
    "# split into training and validation\n",
    "train_df = full_df[full_df[\"date\"].between(86, 375)]\n",
    "valid_df = full_df[full_df[\"date\"].between(425, 500)]\n",
    "\n",
    "# build features and labels\n",
    "features = [f\"feature_{x}\" for x in range(130)]\n",
    "train_X = train_df[features].to_numpy()\n",
    "valid_X = valid_df[features].to_numpy()\n",
    "train_y = train_df[\"resp\"].gt(0.0).astype(np.float32).to_numpy()\n",
    "valid_y = valid_df[\"resp\"].gt(0.0).astype(np.float32).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute and normalize data\n",
    "si = SimpleImputer(strategy=\"mean\")\n",
    "ss = StandardScaler()\n",
    "\n",
    "train_X = si.fit_transform(train_X)\n",
    "train_X = ss.fit_transform(train_X)\n",
    "\n",
    "valid_X = si.transform(valid_X)\n",
    "valid_X = ss.transform(valid_X)\n",
    "\n",
    "# compute R2 scores of PLS model for different\n",
    "# number of PLS components\n",
    "#train_scores = np.empty(130)\n",
    "#valid_scores = np.empty(130)\n",
    "#components = np.arange(1, 131)\n",
    "#for ix, comp in enumerate(components):\n",
    "#    pls = PLSRegression(n_components=comp)\n",
    "#    pls.fit(train_X, train_df[\"resp\"])\n",
    "#    train_scores[ix] = pls.score(train_X, train_df[\"resp\"])\n",
    "#    valid_scores[ix] = pls.score(valid_X, valid_df[\"resp\"])\n",
    "\n",
    "# plot the scores\n",
    "#plt.figure(figsize=(8, 5))\n",
    "#sns.lineplot(x=components, y=train_scores, label=\"Training\")\n",
    "#sns.lineplot(x=components, y=valid_scores, label=\"Validation\")\n",
    "#plt.xlabel(\"Components\")    \n",
    "#plt.ylabel(\"Score\")\n",
    "#plt.grid(True)\n",
    "#plt.show()\n",
    "\n",
    "# refit the model on the number of components a little\n",
    "# after the elbow and transform the data\n",
    "pls = PLSRegression(n_components=40)\n",
    "pls.fit(train_X, train_df[\"resp\"])\n",
    "train_X = pls.transform(train_X)\n",
    "valid_X = pls.transform(valid_X)\n",
    "\n",
    "# save preprocessing pipeline\n",
    "pp = make_pipeline(si, ss, pls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters to optimize on validation set\n",
    "params = {\"max_depth\": [7, 9, 11],\n",
    "          \"learning_rate\": [0.05, 0.1, 0.15],\n",
    "          \"subsample\": [0.6, 0.8, 0.9],\n",
    "          \"colsample_bytree\": [0.5, 0.6, 0.7, 0.8],\n",
    "          # \"reg_alpha\": [20.0],\n",
    "          #\"reg_lambda\": [1.0]\n",
    "         }\n",
    "\n",
    "# early stopping\n",
    "fit_params = {\"early_stopping_rounds\": 10,\n",
    "              \"eval_metric\": \"auc\",\n",
    "              \"eval_set\": [(valid_X, valid_y)]}\n",
    "\n",
    "# base model\n",
    "clf = xgb.XGBClassifier(n_estimators=1000, tree_method=\"gpu_hist\", random_state=13)\n",
    "\n",
    "# setup the data for parameter search\n",
    "full_X = np.vstack([train_X, valid_X])\n",
    "full_y = np.append(train_y, valid_y)\n",
    "test_fold = np.append(np.full(train_X.shape[0], -1, dtype=np.int), np.zeros(valid_X.shape[0], dtype=np.int))\n",
    "ps = PredefinedSplit(test_fold)\n",
    "\n",
    "# find the parameters with best AUC score\n",
    "cv = GridSearchCV(clf, params, cv=ps, scoring=\"roc_auc\", refit=False)\n",
    "cv.fit(full_X, full_y, **fit_params)\n",
    "\n",
    "# show the best parameters\n",
    "print(f\"Highest AUC = {round(cv.best_score_, 4)} with parameters:\")\n",
    "for k, v in cv.best_params_.items():\n",
    "    print(f\"  {k} = {v}\")\n",
    "\n",
    "# need to refit the best model manually params, since the\n",
    "# built-in refitting will refit on full_X, full_y\n",
    "model = xgb.XGBClassifier(n_estimators=1000, tree_method=\"gpu_hist\", random_state=13, **cv.best_params_)\n",
    "model.fit(train_X, train_y, **fit_params)\n",
    "\n",
    "# finally refit the model with the best number of estimators\n",
    "model.set_params(n_estimators=clf.best_iteration)\n",
    "model.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = model.predict_proba(valid_X)\n",
    "probs = probs[:, 1]\n",
    "\n",
    "# precision vs recall\n",
    "precisions, recalls, thresholds = precision_recall_curve(valid_y, probs)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(thresholds, precisions[:-1], \"tab:blue\", label=\"Precision\")\n",
    "plt.plot(thresholds, recalls[:-1], \"tab:orange\", label=\"Recall\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.title(\"Validation Precision/recall at threshold\")\n",
    "plt.axis([0, 1, 0, 1])\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(recalls, precisions, \"tab:blue\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Validation  Precision at recall\")\n",
    "plt.axis([0, 1, 0, 1])\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# ROC curve\n",
    "false_positives, true_positives, thresholds = roc_curve(valid_y, probs)\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(false_positives, true_positives, \"tab:blue\")\n",
    "plt.plot([0, 1], [0, 1], \"tab:gray\")\n",
    "plt.xlabel(\"False positive rate\")\n",
    "plt.ylabel(\"True positive rate\")\n",
    "plt.title(\"Validation ROC curve\")\n",
    "plt.axis([0, 1, 0, 1])\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# AUC\n",
    "print(f\"Validation AUC: {roc_auc_score(valid_y, probs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = model.predict_proba(train_X)\n",
    "probs = probs[:, 1]\n",
    "\n",
    "# precision vs recall\n",
    "precisions, recalls, thresholds = precision_recall_curve(train_y, probs)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(thresholds, precisions[:-1], \"tab:blue\", label=\"Precision\")\n",
    "plt.plot(thresholds, recalls[:-1], \"tab:orange\", label=\"Recall\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.title(\"Training Precision/recall at threshold\")\n",
    "plt.axis([0, 1, 0, 1])\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(recalls, precisions, \"tab:blue\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Training  Precision at recall\")\n",
    "plt.axis([0, 1, 0, 1])\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# ROC curve\n",
    "false_positives, true_positives, thresholds = roc_curve(train_y, probs)\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(false_positives, true_positives, \"tab:blue\")\n",
    "plt.plot([0, 1], [0, 1], \"tab:gray\")\n",
    "plt.xlabel(\"False positive rate\")\n",
    "plt.ylabel(\"True positive rate\")\n",
    "plt.title(\"Training ROC curve\")\n",
    "plt.axis([0, 1, 0, 1])\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# AUC\n",
    "print(f\"Training AUC: {roc_auc_score(train_y, probs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "joblib.dump(pp, \"preprocessor.pkl\")\n",
    "model.save_model(\"model.xgb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
