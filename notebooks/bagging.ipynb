{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging Experiments\n",
    "\n",
    "In this notebook we try to address the overfitting problem of the gradient boosted trees in the `naive.ipynb` notebook by implementing some suggestions from the book by Lopez de Prado. We still only use the features at time $t$ to predict the response at time $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install datatable\n",
    "\n",
    "import os\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import datatable as dt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# location of data files\n",
    "comp_folder = os.path.join(os.pardir, \"input\", \"jane-street-market-prediction\")\n",
    "\n",
    "# read the data with datatables, then convert to pandas (faster)\n",
    "df = dt.fread(os.path.join(comp_folder, \"train.csv\")).to_pandas()\n",
    "df.set_index(\"ts_id\", inplace=True)\n",
    "\n",
    "# reduce memory usage\n",
    "df = df.astype({c: np.float32 for c in df.select_dtypes(include=\"float64\").columns})\n",
    "\n",
    "# split by date, to reduce temporal correlations between training/test\n",
    "train_df = df[df[\"date\"] < 350]\n",
    "test_df = df[df[\"date\"] >= 400]\n",
    "\n",
    "# split into features and target\n",
    "feat_cols = [c for c in train_df.columns if \"feature\" in c]\n",
    "train_X = train_df[feat_cols]\n",
    "test_X = test_df[feat_cols]\n",
    "train_y = train_df[\"resp\"]\n",
    "test_y = test_df[\"resp\"]\n",
    "train_weights = train_df[\"weight\"]\n",
    "test_weights = test_df[\"weight\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z-score the targets\n",
    "train_y = (train_y - train_y.mean()) / train_y.std()\n",
    "\n",
    "# targets as classification problem\n",
    "train_y_pos = train_y.gt(0).astype(int)\n",
    "\n",
    "# replace missing values by median\n",
    "imp = SimpleImputer(strategy=\"median\")\n",
    "flow = imp.fit_transform(train_X)\n",
    "\n",
    "# z-score the features\n",
    "ss = StandardScaler()\n",
    "flow = ss.fit_transform(flow)\n",
    "\n",
    "# rotate features onto directions that cause maximal\n",
    "# variance in the response\n",
    "pls = PLSRegression(n_components=60) # 40 PCA components carry 95% variance\n",
    "pls.fit(flow, train_y)\n",
    "flow = pls.transform(flow)\n",
    "\n",
    "# clf = RandomForestClassifier(n_estimators=1000, \n",
    "#                              max_depth=10,\n",
    "#                              max_features=\"log2\",\n",
    "#                              min_weight_fraction_leaf=0.05,\n",
    "#                              class_weight=\"balanced_subsample\",\n",
    "#                              criterion=\"entropy\",\n",
    "#                              random_state=42\n",
    "#                             )\n",
    "\n",
    "# re-z-score the features for SVM\n",
    "ss2 = StandardScaler()\n",
    "flow = ss2.fit_transform(flow)\n",
    "\n",
    "# bag several support vector classifiers\n",
    "# the classifiers stop early and are trained on restricted\n",
    "# samples/features\n",
    "clf = BaggingClassifier(base_estimator=SVC(max_iter=100000),\n",
    "                        n_estimators=300,\n",
    "                        max_samples=10000,\n",
    "                        max_features=10,\n",
    "                        bootstrap_features=True)\n",
    "\n",
    "tick = time()\n",
    "clf.fit(flow, train_y_pos)\n",
    "tock = time()\n",
    "print(f\"Training took {(tock-tick) // 60} minutes\")\n",
    "\n",
    "pred = clf.predict(flow)\n",
    "\n",
    "# metrics on training data\n",
    "print(\"TRAINING SET:\")\n",
    "print(f\"Confusion matrix:\")\n",
    "print(confusion_matrix(train_y_pos, pred))\n",
    "print(f\"Precision: {precision_score(train_y_pos, pred)}\")\n",
    "print(f\"Recall: {recall_score(train_y_pos, pred)}\")\n",
    "print(f\"F1: {f1_score(train_y_pos, pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow = imp.transform(test_X)\n",
    "flow = ss.transform(flow)\n",
    "flow = pls.transform(flow)\n",
    "flow = ss2.transform(flow)\n",
    "pred = clf.predict(flow)\n",
    "\n",
    "# metrics on test set\n",
    "test_y_pos = test_y.gt(0).astype(int)\n",
    "print(\"TEST SET:\")\n",
    "print(f\"Confusion matrix:\")\n",
    "print(confusion_matrix(test_y_pos, pred))\n",
    "print(f\"Precision: {precision_score(test_y_pos, pred)}\")\n",
    "print(f\"Recall: {recall_score(test_y_pos, pred)}\")\n",
    "print(f\"F1: {f1_score(test_y_pos, pred)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
