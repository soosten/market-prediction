{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jane Street Market Prediction - TPU Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run some setup code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 24.015539,
     "end_time": "2020-10-18T23:33:34.951606",
     "exception": false,
     "start_time": "2020-10-18T23:33:10.936067",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "# set up the TPU\n",
    "tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "tf.config.experimental_connect_to_cluster(tpu)\n",
    "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "tpu_replicas = tpu_strategy.num_replicas_in_sync\n",
    "\n",
    "# get dataset credential from the Google Cloud SDK\n",
    "# and  pass credential to tensorflow\n",
    "# this needs to run after TPU intialization\n",
    "user_secrets = UserSecretsClient()\n",
    "user_credential = user_secrets.get_gcloud_credential()\n",
    "user_secrets.set_tensorflow_credential(user_credential)\n",
    "\n",
    "# set tensorflow's random seed\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 0.2\n",
    "\n",
    "# cross-validation parameters\n",
    "FOLDS = 5\n",
    "HOLDOUT = 4\n",
    "\n",
    "# model parameters\n",
    "WINDOW_SIZE = 3\n",
    "NOISE = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_dataset()` returns a dataset generated from the folds in the list `folds`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 1.588044,
     "end_time": "2020-10-18T23:33:50.745129",
     "exception": false,
     "start_time": "2020-10-18T23:33:49.157085",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dataset(folds, repeat=False, shuffle=False, cache=False):\n",
    "    # load a dictionary mapping feature names to columns\n",
    "    col_file = os.path.join(os.pardir, \"input\",\n",
    "                            \"jane-street-market-prediction-data\",\n",
    "                            \"columns.json\")\n",
    "    with open(col_file) as file:\n",
    "        cols = json.loads(file.read())\n",
    "\n",
    "    # shorthand notation for autotune option\n",
    "    auto = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "    # opens a tf record in filename as a dataset that parses serialized\n",
    "    # tensors and returns sliding windows of WINDOW_SIZE samples\n",
    "    def open_windowed_ds(filename):\n",
    "        ds = tf.data.TFRecordDataset(filename)\n",
    "        ds = ds.map(lambda x: tf.io.parse_tensor(x, tf.float32), num_parallel_calls=auto)\n",
    "        ds = ds.window(WINDOW_SIZE, shift=1, drop_remainder=True)\n",
    "        ds = ds.flat_map(lambda x: x.batch(WINDOW_SIZE))\n",
    "        return ds\n",
    "\n",
    "    # create a dataset with filenames of tf records in files_ds\n",
    "    # then interleave the datasets obtained by calling\n",
    "    # open_windowed_ds(x) on each element of files_ds\n",
    "    data_path = KaggleDatasets().get_gcs_path()\n",
    "    patterns = [data_path + f\"/fold{fold}\" + \"/*.tfrec\" for fold in folds]\n",
    "    files = tf.io.gfile.glob(patterns)\n",
    "    files_ds = tf.data.Dataset.from_tensor_slices(files)\n",
    "    ds = files_ds.interleave(open_windowed_ds, num_parallel_calls=auto)\n",
    "\n",
    "    # filter out any time series spanning multiple dates\n",
    "    def single_date(series):\n",
    "        dates, ix = tf.unique(series[:, cols[\"date\"]])\n",
    "        return tf.equal(tf.size(dates), tf.constant(1))\n",
    "\n",
    "    ds = ds.filter(single_date)\n",
    "\n",
    "    # separate the series into a training sample consisting\n",
    "    # of the features and a label indicating whether the\n",
    "    # response at final time is positive\n",
    "    # need to explicitly reshape the tensors here for things\n",
    "    # to work properly on TPU\n",
    "    def collate(series):\n",
    "        X = series[:, cols[\"feature_0\"]:(cols[\"feature_129\"] + 1)]\n",
    "        y = (1.0 + tf.sign(series[-1, cols[\"resp\"]])) / 2.0\n",
    "        return tf.reshape(X, [WINDOW_SIZE, 130]), tf.reshape(y, [1])\n",
    "    \n",
    "    ds = ds.map(collate, num_parallel_calls=auto)\n",
    "\n",
    "    # if shuffling, allow the dataset to ignore the order for speed\n",
    "    ignore_order = tf.data.Options()\n",
    "    ignore_order.experimental_deterministic = not shuffle\n",
    "    ds = ds.with_options(ignore_order)\n",
    "\n",
    "    # check if the dataset should repeat once exhausted\n",
    "    if repeat:\n",
    "        ds = ds.repeat()\n",
    "    \n",
    "    # check if we should shuffle the dataset\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(4 * BATCH_SIZE)\n",
    "\n",
    "    # set the batch size of the dataset\n",
    "    ds = ds.batch(BATCH_SIZE)\n",
    "    \n",
    "    # check if we should cache the dataset\n",
    "    if cache:\n",
    "        ds = ds.cache()\n",
    "\n",
    "    # prefetch new batches in the background\n",
    "    ds = ds.prefetch(auto)\n",
    "\n",
    "    return ds\n",
    "\n",
    "\n",
    "# load a simpler non-windowed version of training set\n",
    "# to adapt the normalization layer - see comments in\n",
    "# get_dataset() for explanations\n",
    "def get_norm_dataset(folds):\n",
    "    col_file = os.path.join(os.pardir, \"input\",\n",
    "                            \"jane-street-market-prediction-data\",\n",
    "                            \"columns.json\")\n",
    "\n",
    "    with open(col_file) as file:\n",
    "        cols = json.loads(file.read())\n",
    "\n",
    "    def parse(serialized):\n",
    "        tensor = tf.io.parse_tensor(serialized, tf.float32)\n",
    "        tensor = tensor[cols[\"feature_0\"]:(cols[\"feature_129\"] + 1)]\n",
    "        return tf.reshape(tensor, [130])\n",
    "\n",
    "    auto = tf.data.experimental.AUTOTUNE\n",
    "    ignore_order = tf.data.Options()\n",
    "    ignore_order.experimental_deterministic = False\n",
    "\n",
    "    data_path = KaggleDatasets().get_gcs_path()\n",
    "    patterns = [data_path + f\"/fold{fold}\" + \"/*.tfrec\" for fold in folds]\n",
    "    ds = tf.data.TFRecordDataset(tf.io.gfile.glob(patterns), num_parallel_reads=auto)\n",
    "    \n",
    "    ds = ds.with_options(ignore_order)\n",
    "    ds = ds.map(parse, num_parallel_calls=auto)\n",
    "    ds = ds.batch(BATCH_SIZE)\n",
    "    ds = ds.prefetch(auto)\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_debug_dataset(folds, repeat=False, shuffle=False, cache=False):\n",
    "    # load a dictionary mapping feature names to columns\n",
    "    col_file = os.path.join(os.pardir, \"input\",\n",
    "                            \"jane-street-market-prediction-data\",\n",
    "                            \"columns.json\")\n",
    "    with open(col_file) as file:\n",
    "        cols = json.loads(file.read())\n",
    "\n",
    "    # shorthand notation for autotune option\n",
    "    auto = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "    # opens a tf record in filename as a dataset that parses serialized\n",
    "    # tensors and returns sliding windows of WINDOW_SIZE samples\n",
    "    def open_windowed_ds(filename):\n",
    "        ds = tf.data.TFRecordDataset(filename)\n",
    "        ds = ds.map(lambda x: tf.io.parse_tensor(x, tf.float32), num_parallel_calls=auto)\n",
    "        ds = ds.window(WINDOW_SIZE, shift=1, drop_remainder=True)\n",
    "        ds = ds.flat_map(lambda x: x.batch(WINDOW_SIZE))\n",
    "        return ds\n",
    "\n",
    "    # create a dataset with filenames of tf records in files_ds\n",
    "    # then interleave the datasets obtained by calling\n",
    "    # open_windowed_ds(x) on each element of files_ds\n",
    "    data_path = KaggleDatasets().get_gcs_path()\n",
    "    patterns = [data_path + f\"/fold{fold}\" + \"/*.tfrec\" for fold in folds]\n",
    "    files = tf.io.gfile.glob(patterns)\n",
    "    files_ds = tf.data.Dataset.from_tensor_slices(files)\n",
    "    ds = files_ds.interleave(open_windowed_ds, num_parallel_calls=auto)\n",
    "\n",
    "    # filter out any time series spanning multiple dates\n",
    "    def single_date(series):\n",
    "        dates, ix = tf.unique(series[:, cols[\"date\"]])\n",
    "        return tf.equal(tf.size(dates), tf.constant(1))\n",
    "\n",
    "    ds = ds.filter(single_date)\n",
    "\n",
    "    # separate the series into a training sample consisting\n",
    "    # of the features and a label indicating whether the\n",
    "    # response at final time is positive\n",
    "    # need to explicitly reshape the tensors here for things\n",
    "    # to work properly on TPU\n",
    "    def collate(series):\n",
    "        X = series[:, cols[\"feature_0\"]:(cols[\"feature_129\"] + 1)]\n",
    "        y = (1.0 + tf.sign(series[-1, cols[\"resp\"]])) / 2.0\n",
    "        d = series[:, cols[\"date\"]]\n",
    "        s = series[:, cols[\"ts_id\"]]\n",
    "        return tf.reshape(X, [WINDOW_SIZE, 130]), tf.reshape(y, [1]), tf.reshape(d, [1]), tf_reshape(s, [1])\n",
    "    \n",
    "    ds = ds.map(collate, num_parallel_calls=auto)\n",
    "\n",
    "    # if shuffling, allow the dataset to ignore the order for speed\n",
    "    ignore_order = tf.data.Options()\n",
    "    ignore_order.experimental_deterministic = not shuffle\n",
    "    ds = ds.with_options(ignore_order)\n",
    "\n",
    "    # check if the dataset should repeat once exhausted\n",
    "    if repeat:\n",
    "        ds = ds.repeat()\n",
    "    \n",
    "    # check if we should shuffle the dataset\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(4 * BATCH_SIZE)\n",
    "\n",
    "    # set the batch size of the dataset\n",
    "    ds = ds.batch(BATCH_SIZE)\n",
    "    \n",
    "    # check if we should cache the dataset\n",
    "    if cache:\n",
    "        ds = ds.cache()\n",
    "\n",
    "    # prefetch new batches in the background\n",
    "    ds = ds.prefetch(auto)\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_ds = get_debug_datset([0, 1,2,3])\n",
    "\n",
    "for item in db_ds.take(3):\n",
    "    print(X)\n",
    "    print(y)\n",
    "    print(d)\n",
    "    print(s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "papermill": {
   "duration": 5526.40816,
   "end_time": "2020-10-19T01:05:12.285603",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-10-18T23:33:05.877443",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
