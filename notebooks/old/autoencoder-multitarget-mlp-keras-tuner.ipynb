{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2020-12-18T18:52:55.620014Z",
     "iopub.status.busy": "2020-12-18T18:52:55.619339Z",
     "iopub.status.idle": "2020-12-18T18:52:55.757463Z",
     "shell.execute_reply": "2020-12-18T18:52:55.756586Z"
    },
    "papermill": {
     "duration": 0.15606,
     "end_time": "2020-12-18T18:52:55.757611",
     "exception": false,
     "start_time": "2020-12-18T18:52:55.601551",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/rank-gauss/rankGaussTrafo.py\n",
      "/kaggle/input/rank-gauss/rgn.py\n",
      "/kaggle/input/rank-gauss/gauss_rank_scaler.py\n",
      "/kaggle/input/jane-street-market-prediction/example_sample_submission.csv\n",
      "/kaggle/input/jane-street-market-prediction/features.csv\n",
      "/kaggle/input/jane-street-market-prediction/example_test.csv\n",
      "/kaggle/input/jane-street-market-prediction/train.csv\n",
      "/kaggle/input/jane-street-market-prediction/janestreet/competition.cpython-37m-x86_64-linux-gnu.so\n",
      "/kaggle/input/jane-street-market-prediction/janestreet/__init__.py\n",
      "/kaggle/input/jsautoencoder/encoder.hdf5\n",
      "/kaggle/input/jsautoencoder/model_42_0.hdf5\n",
      "/kaggle/input/jsautoencoder/model_42_3.hdf5\n",
      "/kaggle/input/jsautoencoder/model_42_2.hdf5\n",
      "/kaggle/input/jsautoencoder/model_42_1.hdf5\n",
      "/kaggle/input/jsautoencoder/best_hp_42.pkl\n",
      "/kaggle/input/jsautoencoder/model_42_4.hdf5\n",
      "/kaggle/input/jsautoencoder/untitled_project/oracle.json\n",
      "/kaggle/input/jsautoencoder/untitled_project/tuner0.json\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_025eddd8bb564f8fc57a1c005a13aba7/trial.json\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_025eddd8bb564f8fc57a1c005a13aba7/checkpoints/epoch_0/checkpoint.data-00000-of-00001\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_025eddd8bb564f8fc57a1c005a13aba7/checkpoints/epoch_0/checkpoint.index\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_025eddd8bb564f8fc57a1c005a13aba7/checkpoints/epoch_0/checkpoint\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_2c16494fca1ea298f7f6358fc5c8f687/trial.json\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_2c16494fca1ea298f7f6358fc5c8f687/checkpoints/epoch_0/checkpoint.data-00000-of-00001\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_2c16494fca1ea298f7f6358fc5c8f687/checkpoints/epoch_0/checkpoint.index\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_2c16494fca1ea298f7f6358fc5c8f687/checkpoints/epoch_0/checkpoint\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_ad9e5f79eaa503c7c05afbd324bc8ca2/trial.json\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_ad9e5f79eaa503c7c05afbd324bc8ca2/checkpoints/epoch_0/checkpoint.data-00000-of-00001\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_ad9e5f79eaa503c7c05afbd324bc8ca2/checkpoints/epoch_0/checkpoint.index\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_ad9e5f79eaa503c7c05afbd324bc8ca2/checkpoints/epoch_0/checkpoint\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_8ccdaea3c51a036b97a4dd80b58575e3/trial.json\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_8ccdaea3c51a036b97a4dd80b58575e3/checkpoints/epoch_0/checkpoint.data-00000-of-00001\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_8ccdaea3c51a036b97a4dd80b58575e3/checkpoints/epoch_0/checkpoint.index\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_8ccdaea3c51a036b97a4dd80b58575e3/checkpoints/epoch_0/checkpoint\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_66f3a462e2fd5ebad1ba2a00c7e3f305/trial.json\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_66f3a462e2fd5ebad1ba2a00c7e3f305/checkpoints/epoch_0/checkpoint.data-00000-of-00001\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_66f3a462e2fd5ebad1ba2a00c7e3f305/checkpoints/epoch_0/checkpoint.index\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_66f3a462e2fd5ebad1ba2a00c7e3f305/checkpoints/epoch_0/checkpoint\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_174fe3fa254deb48619b6a096a91b630/trial.json\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_174fe3fa254deb48619b6a096a91b630/checkpoints/epoch_0/checkpoint.data-00000-of-00001\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_174fe3fa254deb48619b6a096a91b630/checkpoints/epoch_0/checkpoint.index\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_174fe3fa254deb48619b6a096a91b630/checkpoints/epoch_0/checkpoint\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_c8fcfa682753e5270445069b500c3664/trial.json\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_c8fcfa682753e5270445069b500c3664/checkpoints/epoch_0/checkpoint.data-00000-of-00001\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_c8fcfa682753e5270445069b500c3664/checkpoints/epoch_0/checkpoint.index\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_c8fcfa682753e5270445069b500c3664/checkpoints/epoch_0/checkpoint\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_e8c5cb980bbf49dece145e9c0a415dc4/trial.json\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_e8c5cb980bbf49dece145e9c0a415dc4/checkpoints/epoch_0/checkpoint.data-00000-of-00001\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_e8c5cb980bbf49dece145e9c0a415dc4/checkpoints/epoch_0/checkpoint.index\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_e8c5cb980bbf49dece145e9c0a415dc4/checkpoints/epoch_0/checkpoint\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_2993600529c0dcb1928ae2d0631feea2/trial.json\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_2993600529c0dcb1928ae2d0631feea2/checkpoints/epoch_0/checkpoint.data-00000-of-00001\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_2993600529c0dcb1928ae2d0631feea2/checkpoints/epoch_0/checkpoint.index\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_2993600529c0dcb1928ae2d0631feea2/checkpoints/epoch_0/checkpoint\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_f02e6e8ed5097337d024fd5199b14c8f/trial.json\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_f02e6e8ed5097337d024fd5199b14c8f/checkpoints/epoch_0/checkpoint.data-00000-of-00001\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_f02e6e8ed5097337d024fd5199b14c8f/checkpoints/epoch_0/checkpoint.index\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_f02e6e8ed5097337d024fd5199b14c8f/checkpoints/epoch_0/checkpoint\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_27108cca9c112b08951ed0b8d5db4732/trial.json\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_27108cca9c112b08951ed0b8d5db4732/checkpoints/epoch_0/checkpoint.data-00000-of-00001\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_27108cca9c112b08951ed0b8d5db4732/checkpoints/epoch_0/checkpoint.index\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_27108cca9c112b08951ed0b8d5db4732/checkpoints/epoch_0/checkpoint\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_691bf4e2a9210663230d783718e699c2/trial.json\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_691bf4e2a9210663230d783718e699c2/checkpoints/epoch_0/checkpoint.data-00000-of-00001\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_691bf4e2a9210663230d783718e699c2/checkpoints/epoch_0/checkpoint.index\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_691bf4e2a9210663230d783718e699c2/checkpoints/epoch_0/checkpoint\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_639648e53a649b133578a2b2a316f598/trial.json\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_639648e53a649b133578a2b2a316f598/checkpoints/epoch_0/checkpoint.data-00000-of-00001\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_639648e53a649b133578a2b2a316f598/checkpoints/epoch_0/checkpoint.index\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_639648e53a649b133578a2b2a316f598/checkpoints/epoch_0/checkpoint\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_f855199714f4a2bf5a91d35cbffbbed7/trial.json\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_f855199714f4a2bf5a91d35cbffbbed7/checkpoints/epoch_0/checkpoint.data-00000-of-00001\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_f855199714f4a2bf5a91d35cbffbbed7/checkpoints/epoch_0/checkpoint.index\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_f855199714f4a2bf5a91d35cbffbbed7/checkpoints/epoch_0/checkpoint\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_089bff1e89f0aff19b61b6355bf7f8ee/trial.json\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_089bff1e89f0aff19b61b6355bf7f8ee/checkpoints/epoch_0/checkpoint.data-00000-of-00001\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_089bff1e89f0aff19b61b6355bf7f8ee/checkpoints/epoch_0/checkpoint.index\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_089bff1e89f0aff19b61b6355bf7f8ee/checkpoints/epoch_0/checkpoint\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_95d3d5954aef99dfe3337bbbcbb8ae61/trial.json\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_95d3d5954aef99dfe3337bbbcbb8ae61/checkpoints/epoch_0/checkpoint.data-00000-of-00001\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_95d3d5954aef99dfe3337bbbcbb8ae61/checkpoints/epoch_0/checkpoint.index\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_95d3d5954aef99dfe3337bbbcbb8ae61/checkpoints/epoch_0/checkpoint\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_4a0dfb21da4a0f0ed3eec08031f9bc1d/trial.json\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_4a0dfb21da4a0f0ed3eec08031f9bc1d/checkpoints/epoch_0/checkpoint.data-00000-of-00001\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_4a0dfb21da4a0f0ed3eec08031f9bc1d/checkpoints/epoch_0/checkpoint.index\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_4a0dfb21da4a0f0ed3eec08031f9bc1d/checkpoints/epoch_0/checkpoint\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_2d9ce30a124882de8a8f4c65c7933936/trial.json\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_2d9ce30a124882de8a8f4c65c7933936/checkpoints/epoch_0/checkpoint.data-00000-of-00001\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_2d9ce30a124882de8a8f4c65c7933936/checkpoints/epoch_0/checkpoint.index\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_2d9ce30a124882de8a8f4c65c7933936/checkpoints/epoch_0/checkpoint\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_4553ab0f34663d3f62af0f0a32957ef1/trial.json\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_4553ab0f34663d3f62af0f0a32957ef1/checkpoints/epoch_0/checkpoint.data-00000-of-00001\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_4553ab0f34663d3f62af0f0a32957ef1/checkpoints/epoch_0/checkpoint.index\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_4553ab0f34663d3f62af0f0a32957ef1/checkpoints/epoch_0/checkpoint\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_7f95ea20bdc168b8d3c623e91a75658d/trial.json\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_7f95ea20bdc168b8d3c623e91a75658d/checkpoints/epoch_0/checkpoint.data-00000-of-00001\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_7f95ea20bdc168b8d3c623e91a75658d/checkpoints/epoch_0/checkpoint.index\n",
      "/kaggle/input/jsautoencoder/untitled_project/trial_7f95ea20bdc168b8d3c623e91a75658d/checkpoints/epoch_0/checkpoint\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "        \n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.010604,
     "end_time": "2020-12-18T18:52:55.779615",
     "exception": false,
     "start_time": "2020-12-18T18:52:55.769011",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Autoencoder + MLP\n",
    "The idea of using an autoencoder is the denoise the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-18T18:52:55.807638Z",
     "iopub.status.busy": "2020-12-18T18:52:55.807057Z",
     "iopub.status.idle": "2020-12-18T18:53:01.390632Z",
     "shell.execute_reply": "2020-12-18T18:53:01.389601Z"
    },
    "papermill": {
     "duration": 5.600527,
     "end_time": "2020-12-18T18:53:01.390747",
     "exception": false,
     "start_time": "2020-12-18T18:52:55.790220",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Concatenate, Lambda, GaussianNoise, Activation\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "from tqdm import tqdm\n",
    "from random import choices\n",
    "\n",
    "\n",
    "import kerastuner as kt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.01057,
     "end_time": "2020-12-18T18:53:01.412384",
     "exception": false,
     "start_time": "2020-12-18T18:53:01.401814",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## PurgedGroupTimeSeriesSplit\n",
    "Click the code button to see. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2020-12-18T18:53:01.458186Z",
     "iopub.status.busy": "2020-12-18T18:53:01.456350Z",
     "iopub.status.idle": "2020-12-18T18:53:01.458770Z",
     "shell.execute_reply": "2020-12-18T18:53:01.459184Z"
    },
    "papermill": {
     "duration": 0.036153,
     "end_time": "2020-12-18T18:53:01.459288",
     "exception": false,
     "start_time": "2020-12-18T18:53:01.423135",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
    "from sklearn.utils.validation import _deprecate_positional_args\n",
    "\n",
    "# modified code for group gaps; source\n",
    "# https://github.com/getgaurav2/scikit-learn/blob/d4a3af5cc9da3a76f0266932644b884c99724c57/sklearn/model_selection/_split.py#L2243\n",
    "class PurgedGroupTimeSeriesSplit(_BaseKFold):\n",
    "    \"\"\"Time Series cross-validator variant with non-overlapping groups.\n",
    "    Allows for a gap in groups to avoid potentially leaking info from\n",
    "    train into test if the model has windowed or lag features.\n",
    "    Provides train/test indices to split time series data samples\n",
    "    that are observed at fixed time intervals according to a\n",
    "    third-party provided group.\n",
    "    In each split, test indices must be higher than before, and thus shuffling\n",
    "    in cross validator is inappropriate.\n",
    "    This cross-validation object is a variation of :class:`KFold`.\n",
    "    In the kth split, it returns first k folds as train set and the\n",
    "    (k+1)th fold as test set.\n",
    "    The same group will not appear in two different folds (the number of\n",
    "    distinct groups has to be at least equal to the number of folds).\n",
    "    Note that unlike standard cross-validation methods, successive\n",
    "    training sets are supersets of those that come before them.\n",
    "    Read more in the :ref:`User Guide <cross_validation>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int, default=5\n",
    "        Number of splits. Must be at least 2.\n",
    "    max_train_group_size : int, default=Inf\n",
    "        Maximum group size for a single training set.\n",
    "    group_gap : int, default=None\n",
    "        Gap between train and test\n",
    "    max_test_group_size : int, default=Inf\n",
    "        We discard this number of groups from the end of each train split\n",
    "    \"\"\"\n",
    "\n",
    "    @_deprecate_positional_args\n",
    "    def __init__(self,\n",
    "                 n_splits=5,\n",
    "                 *,\n",
    "                 max_train_group_size=np.inf,\n",
    "                 max_test_group_size=np.inf,\n",
    "                 group_gap=None,\n",
    "                 verbose=False\n",
    "                 ):\n",
    "        super().__init__(n_splits, shuffle=False, random_state=None)\n",
    "        self.max_train_group_size = max_train_group_size\n",
    "        self.group_gap = group_gap\n",
    "        self.max_test_group_size = max_test_group_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "        \"\"\"Generate indices to split data into training and test set.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data, where n_samples is the number of samples\n",
    "            and n_features is the number of features.\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Always ignored, exists for compatibility.\n",
    "        groups : array-like of shape (n_samples,)\n",
    "            Group labels for the samples used while splitting the dataset into\n",
    "            train/test set.\n",
    "        Yields\n",
    "        ------\n",
    "        train : ndarray\n",
    "            The training set indices for that split.\n",
    "        test : ndarray\n",
    "            The testing set indices for that split.\n",
    "        \"\"\"\n",
    "        if groups is None:\n",
    "            raise ValueError(\n",
    "                \"The 'groups' parameter should not be None\")\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        n_samples = _num_samples(X)\n",
    "        n_splits = self.n_splits\n",
    "        group_gap = self.group_gap\n",
    "        max_test_group_size = self.max_test_group_size\n",
    "        max_train_group_size = self.max_train_group_size\n",
    "        n_folds = n_splits + 1\n",
    "        group_dict = {}\n",
    "        u, ind = np.unique(groups, return_index=True)\n",
    "        unique_groups = u[np.argsort(ind)]\n",
    "        n_samples = _num_samples(X)\n",
    "        n_groups = _num_samples(unique_groups)\n",
    "        for idx in np.arange(n_samples):\n",
    "            if (groups[idx] in group_dict):\n",
    "                group_dict[groups[idx]].append(idx)\n",
    "            else:\n",
    "                group_dict[groups[idx]] = [idx]\n",
    "        if n_folds > n_groups:\n",
    "            raise ValueError(\n",
    "                (\"Cannot have number of folds={0} greater than\"\n",
    "                 \" the number of groups={1}\").format(n_folds,\n",
    "                                                     n_groups))\n",
    "\n",
    "        group_test_size = min(n_groups // n_folds, max_test_group_size)\n",
    "        group_test_starts = range(n_groups - n_splits * group_test_size,\n",
    "                                  n_groups, group_test_size)\n",
    "        for group_test_start in group_test_starts:\n",
    "            train_array = []\n",
    "            test_array = []\n",
    "\n",
    "            group_st = max(0, group_test_start - group_gap - max_train_group_size)\n",
    "            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n",
    "                train_array_tmp = group_dict[train_group_idx]\n",
    "                \n",
    "                train_array = np.sort(np.unique(\n",
    "                                      np.concatenate((train_array,\n",
    "                                                      train_array_tmp)),\n",
    "                                      axis=None), axis=None)\n",
    "\n",
    "            train_end = train_array.size\n",
    " \n",
    "            for test_group_idx in unique_groups[group_test_start:\n",
    "                                                group_test_start +\n",
    "                                                group_test_size]:\n",
    "                test_array_tmp = group_dict[test_group_idx]\n",
    "                test_array = np.sort(np.unique(\n",
    "                                              np.concatenate((test_array,\n",
    "                                                              test_array_tmp)),\n",
    "                                     axis=None), axis=None)\n",
    "\n",
    "            test_array  = test_array[group_gap:]\n",
    "            \n",
    "            \n",
    "            if self.verbose > 0:\n",
    "                    pass\n",
    "                    \n",
    "            yield [int(i) for i in train_array], [int(i) for i in test_array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2020-12-18T18:53:01.492993Z",
     "iopub.status.busy": "2020-12-18T18:53:01.492251Z",
     "iopub.status.idle": "2020-12-18T18:53:01.494937Z",
     "shell.execute_reply": "2020-12-18T18:53:01.494524Z"
    },
    "papermill": {
     "duration": 0.024806,
     "end_time": "2020-12-18T18:53:01.495023",
     "exception": false,
     "start_time": "2020-12-18T18:53:01.470217",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CVTuner(kt.engine.tuner.Tuner):\n",
    "    def run_trial(self, trial, X, y, splits, batch_size=32, epochs=1,callbacks=None):\n",
    "        val_losses = []\n",
    "        for train_indices, test_indices in splits:\n",
    "            X_train, X_test = [x[train_indices] for x in X], [x[test_indices] for x in X]\n",
    "            y_train, y_test = [a[train_indices] for a in y], [a[test_indices] for a in y]\n",
    "            if len(X_train) < 2:\n",
    "                X_train = X_train[0]\n",
    "                X_test = X_test[0]\n",
    "            if len(y_train) < 2:\n",
    "                y_train = y_train[0]\n",
    "                y_test = y_test[0]\n",
    "            \n",
    "            model = self.hypermodel.build(trial.hyperparameters)\n",
    "            hist = model.fit(X_train,y_train,\n",
    "                      validation_data=(X_test,y_test),\n",
    "                      epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                      callbacks=callbacks)\n",
    "            \n",
    "            val_losses.append([hist.history[k][-1] for k in hist.history])\n",
    "        val_losses = np.asarray(val_losses)\n",
    "        self.oracle.update_trial(trial.trial_id, {k:np.mean(val_losses[:,i]) for i,k in enumerate(hist.history.keys())})\n",
    "        self.save_model(trial.trial_id, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.010768,
     "end_time": "2020-12-18T18:53:01.516679",
     "exception": false,
     "start_time": "2020-12-18T18:53:01.505911",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Loading the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-18T18:53:01.549550Z",
     "iopub.status.busy": "2020-12-18T18:53:01.548979Z",
     "iopub.status.idle": "2020-12-18T18:55:36.818738Z",
     "shell.execute_reply": "2020-12-18T18:55:36.817920Z"
    },
    "papermill": {
     "duration": 155.29101,
     "end_time": "2020-12-18T18:55:36.818904",
     "exception": false,
     "start_time": "2020-12-18T18:53:01.527894",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAINING = True\n",
    "FOLDS = 5\n",
    "SEED = 42\n",
    "\n",
    "train = pd.read_csv('../input/jane-street-market-prediction/train.csv')\n",
    "train = train.query('date > 85').reset_index(drop = True) \n",
    "train = train.astype({c: np.float32 for c in train.select_dtypes(include='float64').columns}) #limit memory use\n",
    "train.fillna(train.mean(),inplace=True)\n",
    "train = train.query('weight > 0').reset_index(drop = True)\n",
    "train['action'] = (train['resp'] > 0).astype('int')\n",
    "features = [c for c in train.columns if 'feature' in c]\n",
    "\n",
    "resp_cols = ['resp_1', 'resp_2', 'resp_3', 'resp', 'resp_4']\n",
    "\n",
    "X = train[features].values\n",
    "y = np.stack([(train[c] > 0).astype('int') for c in resp_cols]).T #Multitarget\n",
    "\n",
    "f_mean = np.mean(train[features[1:]].values,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017867,
     "end_time": "2020-12-18T18:55:36.855978",
     "exception": false,
     "start_time": "2020-12-18T18:55:36.838111",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Creating the autoencoder. \n",
    "The autoencoder should aid in denoising the data. Based on [this](https://www.semanticscholar.org/paper/Deep-Bottleneck-Classifiers-in-Supervised-Dimension-Parviainen/fb86483f7573f6430fe4597432b0cd3e34b16e43) paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-18T18:55:36.919010Z",
     "iopub.status.busy": "2020-12-18T18:55:36.918131Z",
     "iopub.status.idle": "2020-12-18T18:55:36.922358Z",
     "shell.execute_reply": "2020-12-18T18:55:36.923432Z"
    },
    "papermill": {
     "duration": 0.040397,
     "end_time": "2020-12-18T18:55:36.923638",
     "exception": false,
     "start_time": "2020-12-18T18:55:36.883241",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_autoencoder(input_dim,output_dim,noise=0.05):\n",
    "    i = Input(input_dim)\n",
    "    encoded = BatchNormalization()(i)\n",
    "    encoded = GaussianNoise(noise)(encoded)\n",
    "    encoded = Dense(64,activation='relu')(encoded)\n",
    "    decoded = Dropout(0.2)(encoded)\n",
    "    decoded = Dense(input_dim,name='decoded')(decoded)\n",
    "    x = Dense(32,activation='relu')(decoded)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(output_dim,activation='sigmoid',name='label_output')(x)\n",
    "    \n",
    "    encoder = Model(inputs=i,outputs=encoded)\n",
    "    autoencoder = Model(inputs=i,outputs=[decoded,x])\n",
    "    \n",
    "    autoencoder.compile(optimizer=Adam(0.001),loss={'decoded':'mse','label_output':'binary_crossentropy'})\n",
    "    return autoencoder, encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.018626,
     "end_time": "2020-12-18T18:55:36.962919",
     "exception": false,
     "start_time": "2020-12-18T18:55:36.944293",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Creating the MLP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2020-12-18T18:55:37.007319Z",
     "iopub.status.busy": "2020-12-18T18:55:37.006647Z",
     "iopub.status.idle": "2020-12-18T18:55:37.012219Z",
     "shell.execute_reply": "2020-12-18T18:55:37.013165Z"
    },
    "papermill": {
     "duration": 0.034135,
     "end_time": "2020-12-18T18:55:37.013304",
     "exception": false,
     "start_time": "2020-12-18T18:55:36.979169",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_model(hp,input_dim,output_dim,encoder):\n",
    "    inputs = Input(input_dim)\n",
    "    \n",
    "    x = encoder(inputs)\n",
    "    x = Concatenate()([x,inputs]) #use both raw and encoded features\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(hp.Float('init_dropout',0.0,0.5))(x)\n",
    "    \n",
    "    for i in range(hp.Int('num_layers',1,3)):\n",
    "        x = Dense(hp.Int('num_units_{i}',64,256))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Lambda(tf.keras.activations.swish)(x)\n",
    "        x = Dropout(hp.Float(f'dropout_{i}',0.0,0.5))(x)\n",
    "    x = Dense(output_dim,activation='sigmoid')(x)\n",
    "    model = Model(inputs=inputs,outputs=x)\n",
    "    model.compile(optimizer=Adam(hp.Float('lr',0.00001,0.1,default=0.001)),loss=BinaryCrossentropy(label_smoothing=hp.Float('label_smoothing',0.0,0.1)),metrics=[tf.keras.metrics.AUC(name = 'auc')])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.019319,
     "end_time": "2020-12-18T18:55:37.049864",
     "exception": false,
     "start_time": "2020-12-18T18:55:37.030545",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Defining and training the autoencoder. \n",
    "We add gaussian noise with mean and std from training data. After training we lock the layers in the encoder from further training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-18T18:55:37.154955Z",
     "iopub.status.busy": "2020-12-18T18:55:37.154281Z",
     "iopub.status.idle": "2020-12-18T18:58:07.788579Z",
     "shell.execute_reply": "2020-12-18T18:58:07.787470Z"
    },
    "papermill": {
     "duration": 150.721288,
     "end_time": "2020-12-18T18:58:07.788697",
     "exception": false,
     "start_time": "2020-12-18T18:55:37.067409",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "346/346 [==============================] - 3s 9ms/step - loss: 2.8005 - decoded_loss: 2.0762 - label_output_loss: 0.7244 - val_loss: 1.3856 - val_decoded_loss: 0.6945 - val_label_output_loss: 0.6911\n",
      "Epoch 2/1000\n",
      "346/346 [==============================] - 3s 8ms/step - loss: 1.8519 - decoded_loss: 1.1560 - label_output_loss: 0.6960 - val_loss: 1.2320 - val_decoded_loss: 0.5417 - val_label_output_loss: 0.6903\n",
      "Epoch 3/1000\n",
      "346/346 [==============================] - 2s 7ms/step - loss: 1.7771 - decoded_loss: 1.0850 - label_output_loss: 0.6920 - val_loss: 1.1789 - val_decoded_loss: 0.4889 - val_label_output_loss: 0.6900\n",
      "Epoch 4/1000\n",
      "346/346 [==============================] - 3s 9ms/step - loss: 1.7411 - decoded_loss: 1.0499 - label_output_loss: 0.6912 - val_loss: 1.1454 - val_decoded_loss: 0.4555 - val_label_output_loss: 0.6899\n",
      "Epoch 5/1000\n",
      "346/346 [==============================] - 3s 7ms/step - loss: 1.7230 - decoded_loss: 1.0321 - label_output_loss: 0.6910 - val_loss: 1.1327 - val_decoded_loss: 0.4431 - val_label_output_loss: 0.6896\n",
      "Epoch 6/1000\n",
      "346/346 [==============================] - 3s 8ms/step - loss: 1.7041 - decoded_loss: 1.0132 - label_output_loss: 0.6909 - val_loss: 1.1148 - val_decoded_loss: 0.4254 - val_label_output_loss: 0.6894\n",
      "Epoch 7/1000\n",
      "346/346 [==============================] - 2s 7ms/step - loss: 1.6914 - decoded_loss: 1.0007 - label_output_loss: 0.6908 - val_loss: 1.0954 - val_decoded_loss: 0.4061 - val_label_output_loss: 0.6893\n",
      "Epoch 8/1000\n",
      "346/346 [==============================] - 2s 7ms/step - loss: 1.6841 - decoded_loss: 0.9935 - label_output_loss: 0.6907 - val_loss: 1.0910 - val_decoded_loss: 0.4018 - val_label_output_loss: 0.6892\n",
      "Epoch 9/1000\n",
      "346/346 [==============================] - 2s 7ms/step - loss: 1.6721 - decoded_loss: 0.9815 - label_output_loss: 0.6906 - val_loss: 1.0838 - val_decoded_loss: 0.3946 - val_label_output_loss: 0.6892\n",
      "Epoch 10/1000\n",
      "346/346 [==============================] - 3s 8ms/step - loss: 1.6669 - decoded_loss: 0.9764 - label_output_loss: 0.6906 - val_loss: 1.0762 - val_decoded_loss: 0.3869 - val_label_output_loss: 0.6892\n",
      "Epoch 11/1000\n",
      "346/346 [==============================] - 2s 7ms/step - loss: 1.6610 - decoded_loss: 0.9705 - label_output_loss: 0.6905 - val_loss: 1.0779 - val_decoded_loss: 0.3889 - val_label_output_loss: 0.6890\n",
      "Epoch 12/1000\n",
      "346/346 [==============================] - 2s 7ms/step - loss: 1.6563 - decoded_loss: 0.9659 - label_output_loss: 0.6905 - val_loss: 1.0608 - val_decoded_loss: 0.3718 - val_label_output_loss: 0.6890\n",
      "Epoch 13/1000\n",
      "346/346 [==============================] - 2s 7ms/step - loss: 1.6509 - decoded_loss: 0.9605 - label_output_loss: 0.6905 - val_loss: 1.0531 - val_decoded_loss: 0.3640 - val_label_output_loss: 0.6890\n",
      "Epoch 14/1000\n",
      "346/346 [==============================] - 3s 10ms/step - loss: 1.6478 - decoded_loss: 0.9574 - label_output_loss: 0.6904 - val_loss: 1.0604 - val_decoded_loss: 0.3715 - val_label_output_loss: 0.6889\n",
      "Epoch 15/1000\n",
      "346/346 [==============================] - 3s 7ms/step - loss: 1.6492 - decoded_loss: 0.9589 - label_output_loss: 0.6904 - val_loss: 1.0590 - val_decoded_loss: 0.3702 - val_label_output_loss: 0.6889\n",
      "Epoch 16/1000\n",
      "346/346 [==============================] - 3s 9ms/step - loss: 1.6388 - decoded_loss: 0.9485 - label_output_loss: 0.6903 - val_loss: 1.0567 - val_decoded_loss: 0.3680 - val_label_output_loss: 0.6888\n",
      "Epoch 17/1000\n",
      "346/346 [==============================] - 2s 7ms/step - loss: 1.6447 - decoded_loss: 0.9544 - label_output_loss: 0.6903 - val_loss: 1.0495 - val_decoded_loss: 0.3607 - val_label_output_loss: 0.6889\n",
      "Epoch 18/1000\n",
      "346/346 [==============================] - 3s 8ms/step - loss: 1.6438 - decoded_loss: 0.9536 - label_output_loss: 0.6902 - val_loss: 1.0458 - val_decoded_loss: 0.3569 - val_label_output_loss: 0.6889\n",
      "Epoch 19/1000\n",
      "346/346 [==============================] - 2s 7ms/step - loss: 1.6296 - decoded_loss: 0.9394 - label_output_loss: 0.6902 - val_loss: 1.0467 - val_decoded_loss: 0.3578 - val_label_output_loss: 0.6889\n",
      "Epoch 20/1000\n",
      "346/346 [==============================] - 2s 7ms/step - loss: 1.6382 - decoded_loss: 0.9480 - label_output_loss: 0.6902 - val_loss: 1.0418 - val_decoded_loss: 0.3529 - val_label_output_loss: 0.6889\n",
      "Epoch 21/1000\n",
      "346/346 [==============================] - 3s 7ms/step - loss: 1.6369 - decoded_loss: 0.9467 - label_output_loss: 0.6902 - val_loss: 1.0406 - val_decoded_loss: 0.3519 - val_label_output_loss: 0.6888\n",
      "Epoch 22/1000\n",
      "346/346 [==============================] - 3s 7ms/step - loss: 1.6327 - decoded_loss: 0.9426 - label_output_loss: 0.6901 - val_loss: 1.0427 - val_decoded_loss: 0.3539 - val_label_output_loss: 0.6888\n",
      "Epoch 23/1000\n",
      "346/346 [==============================] - 3s 7ms/step - loss: 1.6331 - decoded_loss: 0.9430 - label_output_loss: 0.6901 - val_loss: 1.0353 - val_decoded_loss: 0.3464 - val_label_output_loss: 0.6889\n",
      "Epoch 24/1000\n",
      "346/346 [==============================] - 2s 7ms/step - loss: 1.6318 - decoded_loss: 0.9417 - label_output_loss: 0.6901 - val_loss: 1.0379 - val_decoded_loss: 0.3491 - val_label_output_loss: 0.6887\n",
      "Epoch 25/1000\n",
      "346/346 [==============================] - 2s 7ms/step - loss: 1.6217 - decoded_loss: 0.9316 - label_output_loss: 0.6901 - val_loss: 1.0312 - val_decoded_loss: 0.3424 - val_label_output_loss: 0.6888\n",
      "Epoch 26/1000\n",
      "346/346 [==============================] - 2s 7ms/step - loss: 1.6261 - decoded_loss: 0.9361 - label_output_loss: 0.6901 - val_loss: 1.0380 - val_decoded_loss: 0.3493 - val_label_output_loss: 0.6887\n",
      "Epoch 27/1000\n",
      "346/346 [==============================] - 3s 8ms/step - loss: 1.6271 - decoded_loss: 0.9371 - label_output_loss: 0.6900 - val_loss: 1.0400 - val_decoded_loss: 0.3510 - val_label_output_loss: 0.6889\n",
      "Epoch 28/1000\n",
      "346/346 [==============================] - 2s 7ms/step - loss: 1.6282 - decoded_loss: 0.9381 - label_output_loss: 0.6901 - val_loss: 1.0336 - val_decoded_loss: 0.3449 - val_label_output_loss: 0.6888\n",
      "Epoch 29/1000\n",
      "346/346 [==============================] - 3s 8ms/step - loss: 1.6297 - decoded_loss: 0.9397 - label_output_loss: 0.6900 - val_loss: 1.0276 - val_decoded_loss: 0.3388 - val_label_output_loss: 0.6888\n",
      "Epoch 30/1000\n",
      "346/346 [==============================] - 2s 7ms/step - loss: 1.6267 - decoded_loss: 0.9368 - label_output_loss: 0.6900 - val_loss: 1.0337 - val_decoded_loss: 0.3449 - val_label_output_loss: 0.6888\n",
      "Epoch 31/1000\n",
      "346/346 [==============================] - 3s 8ms/step - loss: 1.6249 - decoded_loss: 0.9349 - label_output_loss: 0.6900 - val_loss: 1.0316 - val_decoded_loss: 0.3430 - val_label_output_loss: 0.6886\n",
      "Epoch 32/1000\n",
      "346/346 [==============================] - 2s 7ms/step - loss: 1.6252 - decoded_loss: 0.9352 - label_output_loss: 0.6900 - val_loss: 1.0337 - val_decoded_loss: 0.3451 - val_label_output_loss: 0.6887\n",
      "Epoch 33/1000\n",
      "346/346 [==============================] - 3s 8ms/step - loss: 1.6181 - decoded_loss: 0.9282 - label_output_loss: 0.6900 - val_loss: 1.0313 - val_decoded_loss: 0.3428 - val_label_output_loss: 0.6886\n",
      "Epoch 34/1000\n",
      "346/346 [==============================] - 2s 7ms/step - loss: 1.6191 - decoded_loss: 0.9292 - label_output_loss: 0.6900 - val_loss: 1.0265 - val_decoded_loss: 0.3378 - val_label_output_loss: 0.6886\n",
      "Epoch 35/1000\n",
      "346/346 [==============================] - 3s 7ms/step - loss: 1.6186 - decoded_loss: 0.9287 - label_output_loss: 0.6899 - val_loss: 1.0263 - val_decoded_loss: 0.3376 - val_label_output_loss: 0.6888\n",
      "Epoch 36/1000\n",
      "346/346 [==============================] - 3s 7ms/step - loss: 1.6226 - decoded_loss: 0.9327 - label_output_loss: 0.6899 - val_loss: 1.0261 - val_decoded_loss: 0.3373 - val_label_output_loss: 0.6888\n",
      "Epoch 37/1000\n",
      "346/346 [==============================] - 3s 9ms/step - loss: 1.6156 - decoded_loss: 0.9257 - label_output_loss: 0.6900 - val_loss: 1.0276 - val_decoded_loss: 0.3390 - val_label_output_loss: 0.6886\n",
      "Epoch 38/1000\n",
      "346/346 [==============================] - 2s 7ms/step - loss: 1.6137 - decoded_loss: 0.9239 - label_output_loss: 0.6898 - val_loss: 1.0266 - val_decoded_loss: 0.3381 - val_label_output_loss: 0.6886\n",
      "Epoch 39/1000\n",
      "346/346 [==============================] - 3s 7ms/step - loss: 1.6172 - decoded_loss: 0.9273 - label_output_loss: 0.6899 - val_loss: 1.0240 - val_decoded_loss: 0.3353 - val_label_output_loss: 0.6888\n",
      "Epoch 40/1000\n",
      "346/346 [==============================] - 3s 8ms/step - loss: 1.6151 - decoded_loss: 0.9252 - label_output_loss: 0.6899 - val_loss: 1.0238 - val_decoded_loss: 0.3352 - val_label_output_loss: 0.6887\n",
      "Epoch 41/1000\n",
      "346/346 [==============================] - 3s 8ms/step - loss: 1.6210 - decoded_loss: 0.9311 - label_output_loss: 0.6899 - val_loss: 1.0300 - val_decoded_loss: 0.3413 - val_label_output_loss: 0.6887\n",
      "Epoch 42/1000\n",
      "346/346 [==============================] - 3s 8ms/step - loss: 1.6159 - decoded_loss: 0.9260 - label_output_loss: 0.6898 - val_loss: 1.0301 - val_decoded_loss: 0.3415 - val_label_output_loss: 0.6886\n",
      "Epoch 43/1000\n",
      "346/346 [==============================] - 2s 7ms/step - loss: 1.6171 - decoded_loss: 0.9272 - label_output_loss: 0.6899 - val_loss: 1.0218 - val_decoded_loss: 0.3332 - val_label_output_loss: 0.6887\n",
      "Epoch 44/1000\n",
      "346/346 [==============================] - 3s 8ms/step - loss: 1.6173 - decoded_loss: 0.9275 - label_output_loss: 0.6899 - val_loss: 1.0257 - val_decoded_loss: 0.3371 - val_label_output_loss: 0.6886\n",
      "Epoch 45/1000\n",
      "346/346 [==============================] - 2s 7ms/step - loss: 1.6136 - decoded_loss: 0.9237 - label_output_loss: 0.6898 - val_loss: 1.0154 - val_decoded_loss: 0.3267 - val_label_output_loss: 0.6887\n",
      "Epoch 46/1000\n",
      "346/346 [==============================] - 2s 7ms/step - loss: 1.6157 - decoded_loss: 0.9259 - label_output_loss: 0.6899 - val_loss: 1.0207 - val_decoded_loss: 0.3321 - val_label_output_loss: 0.6886\n",
      "Epoch 47/1000\n",
      "346/346 [==============================] - 2s 7ms/step - loss: 1.6132 - decoded_loss: 0.9233 - label_output_loss: 0.6898 - val_loss: 1.0269 - val_decoded_loss: 0.3383 - val_label_output_loss: 0.6886\n",
      "Epoch 48/1000\n",
      "346/346 [==============================] - 3s 8ms/step - loss: 1.6149 - decoded_loss: 0.9251 - label_output_loss: 0.6898 - val_loss: 1.0175 - val_decoded_loss: 0.3288 - val_label_output_loss: 0.6887\n",
      "Epoch 49/1000\n",
      "346/346 [==============================] - 2s 7ms/step - loss: 1.6121 - decoded_loss: 0.9223 - label_output_loss: 0.6898 - val_loss: 1.0234 - val_decoded_loss: 0.3348 - val_label_output_loss: 0.6886\n",
      "Epoch 50/1000\n",
      "346/346 [==============================] - 2s 7ms/step - loss: 1.6217 - decoded_loss: 0.9320 - label_output_loss: 0.6898 - val_loss: 1.0167 - val_decoded_loss: 0.3280 - val_label_output_loss: 0.6886\n",
      "Epoch 51/1000\n",
      "346/346 [==============================] - 2s 7ms/step - loss: 1.6207 - decoded_loss: 0.9308 - label_output_loss: 0.6898 - val_loss: 1.0203 - val_decoded_loss: 0.3316 - val_label_output_loss: 0.6887\n",
      "Epoch 52/1000\n",
      "346/346 [==============================] - 3s 8ms/step - loss: 1.6132 - decoded_loss: 0.9235 - label_output_loss: 0.6898 - val_loss: 1.0191 - val_decoded_loss: 0.3303 - val_label_output_loss: 0.6888\n",
      "Epoch 53/1000\n",
      "346/346 [==============================] - 2s 7ms/step - loss: 1.6100 - decoded_loss: 0.9203 - label_output_loss: 0.6898 - val_loss: 1.0167 - val_decoded_loss: 0.3282 - val_label_output_loss: 0.6886\n",
      "Epoch 54/1000\n",
      "346/346 [==============================] - 3s 8ms/step - loss: 1.6099 - decoded_loss: 0.9201 - label_output_loss: 0.6898 - val_loss: 1.0185 - val_decoded_loss: 0.3300 - val_label_output_loss: 0.6886\n",
      "Epoch 55/1000\n",
      "346/346 [==============================] - 3s 8ms/step - loss: 1.6182 - decoded_loss: 0.9284 - label_output_loss: 0.6898 - val_loss: 1.0288 - val_decoded_loss: 0.3399 - val_label_output_loss: 0.6888\n"
     ]
    }
   ],
   "source": [
    "autoencoder, encoder = create_autoencoder(X.shape[-1],y.shape[-1],noise=0.1)\n",
    "if TRAINING:\n",
    "    autoencoder.fit(X,(X,y),\n",
    "                    epochs=1000,\n",
    "                    batch_size=4096, \n",
    "                    validation_split=0.1,\n",
    "                    callbacks=[EarlyStopping('val_loss',patience=10,restore_best_weights=True)])\n",
    "    encoder.save_weights('./encoder.hdf5')\n",
    "else:\n",
    "    encoder.load_weights('../input/jsautoencoder/encoder.hdf5')\n",
    "encoder.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.717827,
     "end_time": "2020-12-18T18:58:09.231593",
     "exception": false,
     "start_time": "2020-12-18T18:58:08.513766",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Running CV\n",
    "Following [this notebook](https://www.kaggle.com/gogo827jz/jane-street-ffill-xgboost-purgedtimeseriescv) which use 5 PurgedGroupTimeSeriesSplit split on the dates in the training data. \n",
    "\n",
    "We add the locked encoder as the first layer of the MLP. This seems to help in speeding up the submission rather than first predicting using the encoder then using the MLP. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.980419,
     "end_time": "2020-12-18T18:58:10.924072",
     "exception": false,
     "start_time": "2020-12-18T18:58:09.943653",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We use a Baysian Optimizer to find the optimal HPs for out model. 20 trials take about 2 hours on GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-18T18:58:12.360315Z",
     "iopub.status.busy": "2020-12-18T18:58:12.359487Z",
     "iopub.status.idle": "2020-12-18T20:16:06.985201Z",
     "shell.execute_reply": "2020-12-18T20:16:06.984455Z"
    },
    "papermill": {
     "duration": 4675.341563,
     "end_time": "2020-12-18T20:16:06.985366",
     "exception": false,
     "start_time": "2020-12-18T18:58:11.643803",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 20 Complete [00h 02m 26s]\n",
      "val_auc: 0.5435759544372558\n",
      "\n",
      "Best val_auc So Far: 0.5435759544372558\n",
      "Total elapsed time: 01h 12m 16s\n",
      "Epoch 1/100\n",
      "36/36 [==============================] - 1s 30ms/step - loss: 0.7346 - auc: 0.5066 - val_loss: 0.7045 - val_auc: 0.5146\n",
      "Epoch 2/100\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.7102 - auc: 0.5165 - val_loss: 0.6968 - val_auc: 0.5224\n",
      "Epoch 3/100\n",
      "36/36 [==============================] - 0s 13ms/step - loss: 0.7033 - auc: 0.5228 - val_loss: 0.6945 - val_auc: 0.5269\n",
      "Epoch 4/100\n",
      "36/36 [==============================] - 0s 13ms/step - loss: 0.6996 - auc: 0.5266 - val_loss: 0.6933 - val_auc: 0.5307\n",
      "Epoch 5/100\n",
      "36/36 [==============================] - 1s 14ms/step - loss: 0.6974 - auc: 0.5284 - val_loss: 0.6929 - val_auc: 0.5316\n",
      "Epoch 6/100\n",
      "36/36 [==============================] - 0s 13ms/step - loss: 0.6953 - auc: 0.5326 - val_loss: 0.6923 - val_auc: 0.5328\n",
      "Epoch 7/100\n",
      "36/36 [==============================] - 0s 13ms/step - loss: 0.6938 - auc: 0.5366 - val_loss: 0.6920 - val_auc: 0.5345\n",
      "Epoch 8/100\n",
      "36/36 [==============================] - 0s 13ms/step - loss: 0.6930 - auc: 0.5370 - val_loss: 0.6918 - val_auc: 0.5345\n",
      "Epoch 9/100\n",
      "36/36 [==============================] - 0s 13ms/step - loss: 0.6923 - auc: 0.5393 - val_loss: 0.6916 - val_auc: 0.5351\n",
      "Epoch 10/100\n",
      "36/36 [==============================] - 0s 13ms/step - loss: 0.6915 - auc: 0.5409 - val_loss: 0.6914 - val_auc: 0.5358\n",
      "Epoch 11/100\n",
      "36/36 [==============================] - 0s 13ms/step - loss: 0.6907 - auc: 0.5436 - val_loss: 0.6914 - val_auc: 0.5368\n",
      "Epoch 12/100\n",
      "36/36 [==============================] - 0s 13ms/step - loss: 0.6902 - auc: 0.5457 - val_loss: 0.6914 - val_auc: 0.5370\n",
      "Epoch 13/100\n",
      "36/36 [==============================] - 0s 13ms/step - loss: 0.6899 - auc: 0.5456 - val_loss: 0.6913 - val_auc: 0.5372\n",
      "Epoch 14/100\n",
      "36/36 [==============================] - 0s 13ms/step - loss: 0.6893 - auc: 0.5489 - val_loss: 0.6915 - val_auc: 0.5369\n",
      "Epoch 15/100\n",
      "36/36 [==============================] - 0s 13ms/step - loss: 0.6890 - auc: 0.5495 - val_loss: 0.6912 - val_auc: 0.5377\n",
      "Epoch 16/100\n",
      "36/36 [==============================] - 0s 13ms/step - loss: 0.6885 - auc: 0.5513 - val_loss: 0.6913 - val_auc: 0.5372\n",
      "Epoch 17/100\n",
      "36/36 [==============================] - 0s 13ms/step - loss: 0.6881 - auc: 0.5534 - val_loss: 0.6912 - val_auc: 0.5381\n",
      "Epoch 18/100\n",
      "36/36 [==============================] - 0s 13ms/step - loss: 0.6880 - auc: 0.5527 - val_loss: 0.6911 - val_auc: 0.5379\n",
      "Epoch 19/100\n",
      "36/36 [==============================] - 0s 14ms/step - loss: 0.6874 - auc: 0.5556 - val_loss: 0.6912 - val_auc: 0.5384\n",
      "Epoch 20/100\n",
      "36/36 [==============================] - 1s 19ms/step - loss: 0.6872 - auc: 0.5568 - val_loss: 0.6912 - val_auc: 0.5385\n",
      "Epoch 21/100\n",
      "36/36 [==============================] - 0s 14ms/step - loss: 0.6871 - auc: 0.5564 - val_loss: 0.6913 - val_auc: 0.5380\n",
      "Epoch 22/100\n",
      "36/36 [==============================] - 0s 13ms/step - loss: 0.6869 - auc: 0.5572 - val_loss: 0.6912 - val_auc: 0.5388\n",
      "Epoch 23/100\n",
      "36/36 [==============================] - 0s 13ms/step - loss: 0.6866 - auc: 0.5581 - val_loss: 0.6913 - val_auc: 0.5382\n",
      "Epoch 24/100\n",
      "36/36 [==============================] - 0s 13ms/step - loss: 0.6866 - auc: 0.5586 - val_loss: 0.6915 - val_auc: 0.5376\n",
      "Epoch 25/100\n",
      "36/36 [==============================] - 0s 13ms/step - loss: 0.6858 - auc: 0.5614 - val_loss: 0.6913 - val_auc: 0.5382\n",
      "Epoch 26/100\n",
      "36/36 [==============================] - 0s 13ms/step - loss: 0.6858 - auc: 0.5619 - val_loss: 0.6913 - val_auc: 0.5383\n",
      "Epoch 27/100\n",
      "36/36 [==============================] - 0s 13ms/step - loss: 0.6859 - auc: 0.5606 - val_loss: 0.6913 - val_auc: 0.5390\n",
      "Epoch 28/100\n",
      "36/36 [==============================] - 0s 13ms/step - loss: 0.6857 - auc: 0.5616 - val_loss: 0.6913 - val_auc: 0.5384\n",
      "Epoch 29/100\n",
      "36/36 [==============================] - 0s 13ms/step - loss: 0.6852 - auc: 0.5640 - val_loss: 0.6914 - val_auc: 0.5382\n",
      "Epoch 30/100\n",
      "36/36 [==============================] - 0s 13ms/step - loss: 0.6850 - auc: 0.5651 - val_loss: 0.6915 - val_auc: 0.5388\n",
      "Epoch 31/100\n",
      "36/36 [==============================] - 0s 13ms/step - loss: 0.6849 - auc: 0.5653 - val_loss: 0.6915 - val_auc: 0.5384\n",
      "Epoch 32/100\n",
      "36/36 [==============================] - 0s 13ms/step - loss: 0.6846 - auc: 0.5665 - val_loss: 0.6915 - val_auc: 0.5384\n",
      "Epoch 33/100\n",
      "36/36 [==============================] - 0s 13ms/step - loss: 0.6845 - auc: 0.5668 - val_loss: 0.6914 - val_auc: 0.5392\n",
      "Epoch 34/100\n",
      "36/36 [==============================] - 0s 13ms/step - loss: 0.6843 - auc: 0.5673 - val_loss: 0.6915 - val_auc: 0.5388\n",
      "Epoch 35/100\n",
      "36/36 [==============================] - 0s 13ms/step - loss: 0.6840 - auc: 0.5685 - val_loss: 0.6914 - val_auc: 0.5394\n",
      "Epoch 36/100\n",
      "36/36 [==============================] - 0s 13ms/step - loss: 0.6838 - auc: 0.5693 - val_loss: 0.6917 - val_auc: 0.5385\n",
      "Epoch 37/100\n",
      "36/36 [==============================] - 0s 13ms/step - loss: 0.6837 - auc: 0.5695 - val_loss: 0.6916 - val_auc: 0.5381\n",
      "Epoch 38/100\n",
      "36/36 [==============================] - 0s 13ms/step - loss: 0.6835 - auc: 0.5708 - val_loss: 0.6916 - val_auc: 0.5381\n",
      "Epoch 39/100\n",
      "36/36 [==============================] - 0s 13ms/step - loss: 0.6834 - auc: 0.5710 - val_loss: 0.6917 - val_auc: 0.5390\n",
      "Epoch 40/100\n",
      "36/36 [==============================] - 0s 13ms/step - loss: 0.6831 - auc: 0.5714 - val_loss: 0.6917 - val_auc: 0.5386\n",
      "Epoch 41/100\n",
      "36/36 [==============================] - 0s 13ms/step - loss: 0.6827 - auc: 0.5731 - val_loss: 0.6916 - val_auc: 0.5390\n",
      "Epoch 42/100\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.6827 - auc: 0.5738 - val_loss: 0.6917 - val_auc: 0.5388\n",
      "Epoch 43/100\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.6827 - auc: 0.5733 - val_loss: 0.6919 - val_auc: 0.5376\n",
      "Epoch 44/100\n",
      "36/36 [==============================] - 0s 13ms/step - loss: 0.6826 - auc: 0.5734 - val_loss: 0.6918 - val_auc: 0.5385\n",
      "Epoch 45/100\n",
      "36/36 [==============================] - 0s 13ms/step - loss: 0.6823 - auc: 0.5736 - val_loss: 0.6919 - val_auc: 0.5389\n",
      "Epoch 1/3\n",
      "58/58 [==============================] - 0s 9ms/step - loss: 0.6946\n",
      "Epoch 2/3\n",
      "58/58 [==============================] - 0s 7ms/step - loss: 0.6943\n",
      "Epoch 3/3\n",
      "58/58 [==============================] - 0s 5ms/step - loss: 0.6941\n",
      "Epoch 1/100\n",
      "91/91 [==============================] - 1s 14ms/step - loss: 0.7181 - auc: 0.5127 - val_loss: 0.6942 - val_auc: 0.5286\n",
      "Epoch 2/100\n",
      "91/91 [==============================] - 1s 9ms/step - loss: 0.7006 - auc: 0.5230 - val_loss: 0.6916 - val_auc: 0.5359\n",
      "Epoch 3/100\n",
      "91/91 [==============================] - 1s 9ms/step - loss: 0.6964 - auc: 0.5280 - val_loss: 0.6911 - val_auc: 0.5371\n",
      "Epoch 4/100\n",
      "91/91 [==============================] - 1s 9ms/step - loss: 0.6941 - auc: 0.5315 - val_loss: 0.6908 - val_auc: 0.5381\n",
      "Epoch 5/100\n",
      "91/91 [==============================] - 1s 14ms/step - loss: 0.6927 - auc: 0.5345 - val_loss: 0.6906 - val_auc: 0.5395\n",
      "Epoch 6/100\n",
      "91/91 [==============================] - 1s 13ms/step - loss: 0.6918 - auc: 0.5366 - val_loss: 0.6904 - val_auc: 0.5400\n",
      "Epoch 7/100\n",
      "91/91 [==============================] - 1s 11ms/step - loss: 0.6911 - auc: 0.5392 - val_loss: 0.6904 - val_auc: 0.5405\n",
      "Epoch 8/100\n",
      "91/91 [==============================] - 1s 10ms/step - loss: 0.6907 - auc: 0.5410 - val_loss: 0.6904 - val_auc: 0.5404\n",
      "Epoch 9/100\n",
      "91/91 [==============================] - 1s 9ms/step - loss: 0.6903 - auc: 0.5427 - val_loss: 0.6904 - val_auc: 0.5405\n",
      "Epoch 10/100\n",
      "91/91 [==============================] - 1s 10ms/step - loss: 0.6898 - auc: 0.5448 - val_loss: 0.6904 - val_auc: 0.5406\n",
      "Epoch 11/100\n",
      "91/91 [==============================] - 1s 10ms/step - loss: 0.6896 - auc: 0.5459 - val_loss: 0.6902 - val_auc: 0.5418\n",
      "Epoch 12/100\n",
      "91/91 [==============================] - 1s 10ms/step - loss: 0.6894 - auc: 0.5469 - val_loss: 0.6902 - val_auc: 0.5422\n",
      "Epoch 13/100\n",
      "91/91 [==============================] - 1s 10ms/step - loss: 0.6891 - auc: 0.5482 - val_loss: 0.6902 - val_auc: 0.5421\n",
      "Epoch 14/100\n",
      "91/91 [==============================] - 1s 9ms/step - loss: 0.6890 - auc: 0.5483 - val_loss: 0.6902 - val_auc: 0.5421\n",
      "Epoch 15/100\n",
      "91/91 [==============================] - 1s 10ms/step - loss: 0.6888 - auc: 0.5495 - val_loss: 0.6902 - val_auc: 0.5423\n",
      "Epoch 16/100\n",
      "91/91 [==============================] - 1s 9ms/step - loss: 0.6886 - auc: 0.5508 - val_loss: 0.6903 - val_auc: 0.5421\n",
      "Epoch 17/100\n",
      "91/91 [==============================] - 1s 12ms/step - loss: 0.6883 - auc: 0.5520 - val_loss: 0.6903 - val_auc: 0.5418\n",
      "Epoch 18/100\n",
      "91/91 [==============================] - 1s 10ms/step - loss: 0.6883 - auc: 0.5521 - val_loss: 0.6904 - val_auc: 0.5418\n",
      "Epoch 19/100\n",
      "91/91 [==============================] - 1s 9ms/step - loss: 0.6881 - auc: 0.5530 - val_loss: 0.6903 - val_auc: 0.5422\n",
      "Epoch 20/100\n",
      "91/91 [==============================] - 1s 9ms/step - loss: 0.6879 - auc: 0.5541 - val_loss: 0.6904 - val_auc: 0.5416\n",
      "Epoch 21/100\n",
      "91/91 [==============================] - 1s 9ms/step - loss: 0.6877 - auc: 0.5547 - val_loss: 0.6906 - val_auc: 0.5415\n",
      "Epoch 22/100\n",
      "91/91 [==============================] - 1s 10ms/step - loss: 0.6876 - auc: 0.5553 - val_loss: 0.6904 - val_auc: 0.5425\n",
      "Epoch 23/100\n",
      "91/91 [==============================] - 1s 10ms/step - loss: 0.6874 - auc: 0.5567 - val_loss: 0.6902 - val_auc: 0.5435\n",
      "Epoch 24/100\n",
      "91/91 [==============================] - 1s 9ms/step - loss: 0.6873 - auc: 0.5567 - val_loss: 0.6903 - val_auc: 0.5432\n",
      "Epoch 25/100\n",
      "91/91 [==============================] - 1s 9ms/step - loss: 0.6872 - auc: 0.5571 - val_loss: 0.6903 - val_auc: 0.5429\n",
      "Epoch 26/100\n",
      "91/91 [==============================] - 1s 9ms/step - loss: 0.6870 - auc: 0.5578 - val_loss: 0.6904 - val_auc: 0.5425\n",
      "Epoch 27/100\n",
      "91/91 [==============================] - 1s 9ms/step - loss: 0.6868 - auc: 0.5583 - val_loss: 0.6906 - val_auc: 0.5418\n",
      "Epoch 28/100\n",
      "91/91 [==============================] - 1s 9ms/step - loss: 0.6868 - auc: 0.5587 - val_loss: 0.6905 - val_auc: 0.5436\n",
      "Epoch 29/100\n",
      "91/91 [==============================] - 1s 10ms/step - loss: 0.6865 - auc: 0.5596 - val_loss: 0.6905 - val_auc: 0.5429\n",
      "Epoch 30/100\n",
      "91/91 [==============================] - 1s 11ms/step - loss: 0.6864 - auc: 0.5601 - val_loss: 0.6907 - val_auc: 0.5419\n",
      "Epoch 31/100\n",
      "91/91 [==============================] - 1s 9ms/step - loss: 0.6863 - auc: 0.5609 - val_loss: 0.6907 - val_auc: 0.5417\n",
      "Epoch 32/100\n",
      "91/91 [==============================] - 1s 13ms/step - loss: 0.6859 - auc: 0.5624 - val_loss: 0.6908 - val_auc: 0.5416\n",
      "Epoch 33/100\n",
      "91/91 [==============================] - 1s 10ms/step - loss: 0.6860 - auc: 0.5622 - val_loss: 0.6910 - val_auc: 0.5415\n",
      "Epoch 34/100\n",
      "91/91 [==============================] - 1s 9ms/step - loss: 0.6857 - auc: 0.5630 - val_loss: 0.6907 - val_auc: 0.5431\n",
      "Epoch 35/100\n",
      "91/91 [==============================] - 1s 9ms/step - loss: 0.6858 - auc: 0.5625 - val_loss: 0.6909 - val_auc: 0.5417\n",
      "Epoch 36/100\n",
      "91/91 [==============================] - 1s 9ms/step - loss: 0.6856 - auc: 0.5631 - val_loss: 0.6910 - val_auc: 0.5419\n",
      "Epoch 37/100\n",
      "91/91 [==============================] - 1s 9ms/step - loss: 0.6855 - auc: 0.5641 - val_loss: 0.6910 - val_auc: 0.5426\n",
      "Epoch 38/100\n",
      "91/91 [==============================] - 1s 13ms/step - loss: 0.6854 - auc: 0.5638 - val_loss: 0.6908 - val_auc: 0.5430\n",
      "Epoch 1/3\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 0.6922\n",
      "Epoch 2/3\n",
      "65/65 [==============================] - 0s 5ms/step - loss: 0.6920\n",
      "Epoch 3/3\n",
      "65/65 [==============================] - 0s 6ms/step - loss: 0.6920\n",
      "Epoch 1/100\n",
      "152/152 [==============================] - 2s 11ms/step - loss: 0.7106 - auc: 0.5160 - val_loss: 0.6923 - val_auc: 0.5344\n",
      "Epoch 2/100\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.6963 - auc: 0.5270 - val_loss: 0.6909 - val_auc: 0.5392\n",
      "Epoch 3/100\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.6934 - auc: 0.5323 - val_loss: 0.6905 - val_auc: 0.5399\n",
      "Epoch 4/100\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.6919 - auc: 0.5361 - val_loss: 0.6903 - val_auc: 0.5416\n",
      "Epoch 5/100\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.6911 - auc: 0.5390 - val_loss: 0.6902 - val_auc: 0.5426\n",
      "Epoch 6/100\n",
      "152/152 [==============================] - 1s 9ms/step - loss: 0.6905 - auc: 0.5418 - val_loss: 0.6902 - val_auc: 0.5426\n",
      "Epoch 7/100\n",
      "152/152 [==============================] - 2s 10ms/step - loss: 0.6902 - auc: 0.5433 - val_loss: 0.6901 - val_auc: 0.5429\n",
      "Epoch 8/100\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.6900 - auc: 0.5440 - val_loss: 0.6900 - val_auc: 0.5434\n",
      "Epoch 9/100\n",
      "152/152 [==============================] - 1s 9ms/step - loss: 0.6896 - auc: 0.5459 - val_loss: 0.6900 - val_auc: 0.5441\n",
      "Epoch 10/100\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.6893 - auc: 0.5480 - val_loss: 0.6901 - val_auc: 0.5436\n",
      "Epoch 11/100\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.6893 - auc: 0.5477 - val_loss: 0.6899 - val_auc: 0.5443\n",
      "Epoch 12/100\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.6890 - auc: 0.5495 - val_loss: 0.6898 - val_auc: 0.5450\n",
      "Epoch 13/100\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.6889 - auc: 0.5496 - val_loss: 0.6900 - val_auc: 0.5444\n",
      "Epoch 14/100\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.6886 - auc: 0.5516 - val_loss: 0.6899 - val_auc: 0.5447\n",
      "Epoch 15/100\n",
      "152/152 [==============================] - 2s 10ms/step - loss: 0.6885 - auc: 0.5521 - val_loss: 0.6898 - val_auc: 0.5455\n",
      "Epoch 16/100\n",
      "152/152 [==============================] - 2s 11ms/step - loss: 0.6884 - auc: 0.5520 - val_loss: 0.6900 - val_auc: 0.5445\n",
      "Epoch 17/100\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.6882 - auc: 0.5536 - val_loss: 0.6898 - val_auc: 0.5455\n",
      "Epoch 18/100\n",
      "152/152 [==============================] - 2s 11ms/step - loss: 0.6881 - auc: 0.5537 - val_loss: 0.6900 - val_auc: 0.5455\n",
      "Epoch 19/100\n",
      "152/152 [==============================] - 1s 9ms/step - loss: 0.6880 - auc: 0.5540 - val_loss: 0.6900 - val_auc: 0.5450\n",
      "Epoch 20/100\n",
      "152/152 [==============================] - 1s 9ms/step - loss: 0.6878 - auc: 0.5556 - val_loss: 0.6899 - val_auc: 0.5455\n",
      "Epoch 21/100\n",
      "152/152 [==============================] - 1s 10ms/step - loss: 0.6876 - auc: 0.5563 - val_loss: 0.6899 - val_auc: 0.5454\n",
      "Epoch 22/100\n",
      "152/152 [==============================] - 1s 9ms/step - loss: 0.6874 - auc: 0.5571 - val_loss: 0.6899 - val_auc: 0.5461\n",
      "Epoch 23/100\n",
      "152/152 [==============================] - 2s 10ms/step - loss: 0.6874 - auc: 0.5569 - val_loss: 0.6898 - val_auc: 0.5463\n",
      "Epoch 24/100\n",
      "152/152 [==============================] - 1s 9ms/step - loss: 0.6873 - auc: 0.5574 - val_loss: 0.6900 - val_auc: 0.5456\n",
      "Epoch 25/100\n",
      "152/152 [==============================] - 1s 9ms/step - loss: 0.6872 - auc: 0.5578 - val_loss: 0.6899 - val_auc: 0.5462\n",
      "Epoch 26/100\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.6871 - auc: 0.5583 - val_loss: 0.6900 - val_auc: 0.5458\n",
      "Epoch 27/100\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.6870 - auc: 0.5587 - val_loss: 0.6901 - val_auc: 0.5458\n",
      "Epoch 28/100\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.6868 - auc: 0.5595 - val_loss: 0.6900 - val_auc: 0.5462\n",
      "Epoch 29/100\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.6868 - auc: 0.5594 - val_loss: 0.6901 - val_auc: 0.5460\n",
      "Epoch 30/100\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.6867 - auc: 0.5600 - val_loss: 0.6902 - val_auc: 0.5448\n",
      "Epoch 31/100\n",
      "152/152 [==============================] - 1s 10ms/step - loss: 0.6865 - auc: 0.5606 - val_loss: 0.6901 - val_auc: 0.5461\n",
      "Epoch 32/100\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.6864 - auc: 0.5610 - val_loss: 0.6901 - val_auc: 0.5461\n",
      "Epoch 33/100\n",
      "152/152 [==============================] - 1s 9ms/step - loss: 0.6863 - auc: 0.5616 - val_loss: 0.6901 - val_auc: 0.5467\n",
      "Epoch 34/100\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.6862 - auc: 0.5620 - val_loss: 0.6900 - val_auc: 0.5468\n",
      "Epoch 35/100\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.6862 - auc: 0.5619 - val_loss: 0.6901 - val_auc: 0.5468\n",
      "Epoch 36/100\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.6860 - auc: 0.5627 - val_loss: 0.6901 - val_auc: 0.5464\n",
      "Epoch 37/100\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.6860 - auc: 0.5629 - val_loss: 0.6901 - val_auc: 0.5466\n",
      "Epoch 38/100\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.6859 - auc: 0.5633 - val_loss: 0.6904 - val_auc: 0.5455\n",
      "Epoch 39/100\n",
      "152/152 [==============================] - 2s 11ms/step - loss: 0.6858 - auc: 0.5633 - val_loss: 0.6902 - val_auc: 0.5461\n",
      "Epoch 40/100\n",
      "152/152 [==============================] - 1s 10ms/step - loss: 0.6857 - auc: 0.5639 - val_loss: 0.6901 - val_auc: 0.5469\n",
      "Epoch 41/100\n",
      "152/152 [==============================] - 1s 9ms/step - loss: 0.6855 - auc: 0.5646 - val_loss: 0.6903 - val_auc: 0.5468\n",
      "Epoch 42/100\n",
      "152/152 [==============================] - 2s 11ms/step - loss: 0.6854 - auc: 0.5648 - val_loss: 0.6904 - val_auc: 0.5462\n",
      "Epoch 43/100\n",
      "152/152 [==============================] - 1s 9ms/step - loss: 0.6854 - auc: 0.5648 - val_loss: 0.6904 - val_auc: 0.5460\n",
      "Epoch 44/100\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.6853 - auc: 0.5654 - val_loss: 0.6904 - val_auc: 0.5459\n",
      "Epoch 45/100\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.6853 - auc: 0.5654 - val_loss: 0.6905 - val_auc: 0.5450\n",
      "Epoch 46/100\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.6851 - auc: 0.5662 - val_loss: 0.6904 - val_auc: 0.5455\n",
      "Epoch 47/100\n",
      "152/152 [==============================] - 2s 10ms/step - loss: 0.6850 - auc: 0.5667 - val_loss: 0.6903 - val_auc: 0.5467\n",
      "Epoch 48/100\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.6850 - auc: 0.5668 - val_loss: 0.6906 - val_auc: 0.5458\n",
      "Epoch 49/100\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.6849 - auc: 0.5671 - val_loss: 0.6905 - val_auc: 0.5461\n",
      "Epoch 50/100\n",
      "152/152 [==============================] - 1s 8ms/step - loss: 0.6848 - auc: 0.5672 - val_loss: 0.6904 - val_auc: 0.5462\n",
      "Epoch 1/3\n",
      "66/66 [==============================] - 0s 5ms/step - loss: 0.6913\n",
      "Epoch 2/3\n",
      "66/66 [==============================] - 0s 5ms/step - loss: 0.6912\n",
      "Epoch 3/3\n",
      "66/66 [==============================] - 0s 5ms/step - loss: 0.6908\n",
      "Epoch 1/100\n",
      "218/218 [==============================] - 2s 11ms/step - loss: 0.7066 - auc: 0.5178 - val_loss: 0.6918 - val_auc: 0.5342\n",
      "Epoch 2/100\n",
      "218/218 [==============================] - 2s 8ms/step - loss: 0.6942 - auc: 0.5294 - val_loss: 0.6909 - val_auc: 0.5371\n",
      "Epoch 3/100\n",
      "218/218 [==============================] - 2s 8ms/step - loss: 0.6920 - auc: 0.5350 - val_loss: 0.6907 - val_auc: 0.5379\n",
      "Epoch 4/100\n",
      "218/218 [==============================] - 2s 8ms/step - loss: 0.6910 - auc: 0.5383 - val_loss: 0.6907 - val_auc: 0.5377\n",
      "Epoch 5/100\n",
      "218/218 [==============================] - 2s 8ms/step - loss: 0.6905 - auc: 0.5410 - val_loss: 0.6907 - val_auc: 0.5380\n",
      "Epoch 6/100\n",
      "218/218 [==============================] - 2s 10ms/step - loss: 0.6901 - auc: 0.5430 - val_loss: 0.6905 - val_auc: 0.5391\n",
      "Epoch 7/100\n",
      "218/218 [==============================] - 3s 12ms/step - loss: 0.6898 - auc: 0.5445 - val_loss: 0.6907 - val_auc: 0.5382\n",
      "Epoch 8/100\n",
      "218/218 [==============================] - 2s 8ms/step - loss: 0.6897 - auc: 0.5455 - val_loss: 0.6907 - val_auc: 0.5388\n",
      "Epoch 9/100\n",
      "218/218 [==============================] - 2s 8ms/step - loss: 0.6894 - auc: 0.5472 - val_loss: 0.6904 - val_auc: 0.5394\n",
      "Epoch 10/100\n",
      "218/218 [==============================] - 2s 8ms/step - loss: 0.6893 - auc: 0.5480 - val_loss: 0.6907 - val_auc: 0.5385\n",
      "Epoch 11/100\n",
      "218/218 [==============================] - 2s 8ms/step - loss: 0.6891 - auc: 0.5490 - val_loss: 0.6904 - val_auc: 0.5403\n",
      "Epoch 12/100\n",
      "218/218 [==============================] - 2s 9ms/step - loss: 0.6889 - auc: 0.5497 - val_loss: 0.6905 - val_auc: 0.5402\n",
      "Epoch 13/100\n",
      "218/218 [==============================] - 2s 8ms/step - loss: 0.6888 - auc: 0.5503 - val_loss: 0.6907 - val_auc: 0.5393\n",
      "Epoch 14/100\n",
      "218/218 [==============================] - 2s 8ms/step - loss: 0.6886 - auc: 0.5517 - val_loss: 0.6906 - val_auc: 0.5399\n",
      "Epoch 15/100\n",
      "218/218 [==============================] - 2s 8ms/step - loss: 0.6885 - auc: 0.5516 - val_loss: 0.6904 - val_auc: 0.5412\n",
      "Epoch 16/100\n",
      "218/218 [==============================] - 2s 8ms/step - loss: 0.6884 - auc: 0.5524 - val_loss: 0.6904 - val_auc: 0.5410\n",
      "Epoch 17/100\n",
      "218/218 [==============================] - 2s 8ms/step - loss: 0.6882 - auc: 0.5535 - val_loss: 0.6904 - val_auc: 0.5410\n",
      "Epoch 18/100\n",
      "218/218 [==============================] - 2s 8ms/step - loss: 0.6881 - auc: 0.5538 - val_loss: 0.6904 - val_auc: 0.5425\n",
      "Epoch 19/100\n",
      "218/218 [==============================] - 2s 9ms/step - loss: 0.6880 - auc: 0.5544 - val_loss: 0.6906 - val_auc: 0.5410\n",
      "Epoch 20/100\n",
      "218/218 [==============================] - 2s 8ms/step - loss: 0.6879 - auc: 0.5552 - val_loss: 0.6904 - val_auc: 0.5421\n",
      "Epoch 21/100\n",
      "218/218 [==============================] - 2s 8ms/step - loss: 0.6878 - auc: 0.5553 - val_loss: 0.6906 - val_auc: 0.5411\n",
      "Epoch 22/100\n",
      "218/218 [==============================] - 2s 8ms/step - loss: 0.6876 - auc: 0.5563 - val_loss: 0.6906 - val_auc: 0.5408\n",
      "Epoch 23/100\n",
      "218/218 [==============================] - 2s 8ms/step - loss: 0.6876 - auc: 0.5562 - val_loss: 0.6905 - val_auc: 0.5425\n",
      "Epoch 24/100\n",
      "218/218 [==============================] - 2s 8ms/step - loss: 0.6875 - auc: 0.5567 - val_loss: 0.6907 - val_auc: 0.5417\n",
      "Epoch 25/100\n",
      "218/218 [==============================] - 3s 14ms/step - loss: 0.6874 - auc: 0.5573 - val_loss: 0.6907 - val_auc: 0.5417\n",
      "Epoch 26/100\n",
      "218/218 [==============================] - 2s 9ms/step - loss: 0.6872 - auc: 0.5582 - val_loss: 0.6907 - val_auc: 0.5415\n",
      "Epoch 27/100\n",
      "218/218 [==============================] - 2s 8ms/step - loss: 0.6872 - auc: 0.5583 - val_loss: 0.6908 - val_auc: 0.5422\n",
      "Epoch 28/100\n",
      "218/218 [==============================] - 2s 8ms/step - loss: 0.6871 - auc: 0.5585 - val_loss: 0.6906 - val_auc: 0.5428\n",
      "Epoch 29/100\n",
      "218/218 [==============================] - 2s 8ms/step - loss: 0.6870 - auc: 0.5589 - val_loss: 0.6910 - val_auc: 0.5421\n",
      "Epoch 30/100\n",
      "218/218 [==============================] - 2s 9ms/step - loss: 0.6870 - auc: 0.5589 - val_loss: 0.6909 - val_auc: 0.5415\n",
      "Epoch 31/100\n",
      "218/218 [==============================] - 2s 8ms/step - loss: 0.6867 - auc: 0.5597 - val_loss: 0.6908 - val_auc: 0.5423\n",
      "Epoch 32/100\n",
      "218/218 [==============================] - 2s 8ms/step - loss: 0.6867 - auc: 0.5602 - val_loss: 0.6909 - val_auc: 0.5426\n",
      "Epoch 33/100\n",
      "218/218 [==============================] - 2s 8ms/step - loss: 0.6867 - auc: 0.5601 - val_loss: 0.6909 - val_auc: 0.5429\n",
      "Epoch 34/100\n",
      "218/218 [==============================] - 2s 8ms/step - loss: 0.6865 - auc: 0.5609 - val_loss: 0.6910 - val_auc: 0.5425\n",
      "Epoch 35/100\n",
      "218/218 [==============================] - 2s 8ms/step - loss: 0.6865 - auc: 0.5612 - val_loss: 0.6907 - val_auc: 0.5428\n",
      "Epoch 36/100\n",
      "218/218 [==============================] - 2s 9ms/step - loss: 0.6865 - auc: 0.5607 - val_loss: 0.6908 - val_auc: 0.5434\n",
      "Epoch 37/100\n",
      "218/218 [==============================] - 2s 8ms/step - loss: 0.6864 - auc: 0.5612 - val_loss: 0.6910 - val_auc: 0.5424\n",
      "Epoch 38/100\n",
      "218/218 [==============================] - 2s 10ms/step - loss: 0.6863 - auc: 0.5616 - val_loss: 0.6908 - val_auc: 0.5444\n",
      "Epoch 39/100\n",
      "218/218 [==============================] - 2s 8ms/step - loss: 0.6862 - auc: 0.5623 - val_loss: 0.6910 - val_auc: 0.5433\n",
      "Epoch 40/100\n",
      "218/218 [==============================] - 2s 9ms/step - loss: 0.6861 - auc: 0.5622 - val_loss: 0.6910 - val_auc: 0.5438\n",
      "Epoch 41/100\n",
      "218/218 [==============================] - 2s 8ms/step - loss: 0.6859 - auc: 0.5628 - val_loss: 0.6910 - val_auc: 0.5431\n",
      "Epoch 42/100\n",
      "218/218 [==============================] - 3s 12ms/step - loss: 0.6859 - auc: 0.5632 - val_loss: 0.6912 - val_auc: 0.5423\n",
      "Epoch 43/100\n",
      "218/218 [==============================] - 2s 8ms/step - loss: 0.6858 - auc: 0.5632 - val_loss: 0.6913 - val_auc: 0.5428\n",
      "Epoch 44/100\n",
      "218/218 [==============================] - 2s 8ms/step - loss: 0.6859 - auc: 0.5628 - val_loss: 0.6911 - val_auc: 0.5440\n",
      "Epoch 45/100\n",
      "218/218 [==============================] - 2s 8ms/step - loss: 0.6857 - auc: 0.5639 - val_loss: 0.6912 - val_auc: 0.5433\n",
      "Epoch 46/100\n",
      "218/218 [==============================] - 2s 8ms/step - loss: 0.6856 - auc: 0.5642 - val_loss: 0.6914 - val_auc: 0.5424\n",
      "Epoch 47/100\n",
      "218/218 [==============================] - 2s 8ms/step - loss: 0.6855 - auc: 0.5646 - val_loss: 0.6915 - val_auc: 0.5422\n",
      "Epoch 48/100\n",
      "218/218 [==============================] - 2s 9ms/step - loss: 0.6856 - auc: 0.5643 - val_loss: 0.6914 - val_auc: 0.5429\n",
      "Epoch 1/3\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.6917\n",
      "Epoch 2/3\n",
      "69/69 [==============================] - 0s 6ms/step - loss: 0.6915\n",
      "Epoch 3/3\n",
      "69/69 [==============================] - 0s 5ms/step - loss: 0.6914\n",
      "Epoch 1/100\n",
      "287/287 [==============================] - 3s 9ms/step - loss: 0.7046 - auc: 0.5186 - val_loss: 0.6901 - val_auc: 0.5430\n",
      "Epoch 2/100\n",
      "287/287 [==============================] - 2s 8ms/step - loss: 0.6938 - auc: 0.5296 - val_loss: 0.6896 - val_auc: 0.5459\n",
      "Epoch 3/100\n",
      "287/287 [==============================] - 2s 8ms/step - loss: 0.6919 - auc: 0.5344 - val_loss: 0.6895 - val_auc: 0.5467\n",
      "Epoch 4/100\n",
      "287/287 [==============================] - 2s 8ms/step - loss: 0.6910 - auc: 0.5381 - val_loss: 0.6894 - val_auc: 0.5483\n",
      "Epoch 5/100\n",
      "287/287 [==============================] - 2s 8ms/step - loss: 0.6905 - auc: 0.5405 - val_loss: 0.6892 - val_auc: 0.5489\n",
      "Epoch 6/100\n",
      "287/287 [==============================] - 4s 13ms/step - loss: 0.6901 - auc: 0.5424 - val_loss: 0.6889 - val_auc: 0.5510\n",
      "Epoch 7/100\n",
      "287/287 [==============================] - 2s 8ms/step - loss: 0.6899 - auc: 0.5443 - val_loss: 0.6888 - val_auc: 0.5514\n",
      "Epoch 8/100\n",
      "287/287 [==============================] - 2s 8ms/step - loss: 0.6897 - auc: 0.5453 - val_loss: 0.6888 - val_auc: 0.5508\n",
      "Epoch 9/100\n",
      "287/287 [==============================] - 2s 8ms/step - loss: 0.6895 - auc: 0.5461 - val_loss: 0.6889 - val_auc: 0.5508\n",
      "Epoch 10/100\n",
      "287/287 [==============================] - 3s 9ms/step - loss: 0.6894 - auc: 0.5471 - val_loss: 0.6887 - val_auc: 0.5513\n",
      "Epoch 11/100\n",
      "287/287 [==============================] - 2s 8ms/step - loss: 0.6892 - auc: 0.5483 - val_loss: 0.6888 - val_auc: 0.5508\n",
      "Epoch 12/100\n",
      "287/287 [==============================] - 2s 8ms/step - loss: 0.6891 - auc: 0.5487 - val_loss: 0.6888 - val_auc: 0.5501\n",
      "Epoch 13/100\n",
      "287/287 [==============================] - 2s 8ms/step - loss: 0.6890 - auc: 0.5493 - val_loss: 0.6887 - val_auc: 0.5510\n",
      "Epoch 14/100\n",
      "287/287 [==============================] - 3s 9ms/step - loss: 0.6889 - auc: 0.5500 - val_loss: 0.6887 - val_auc: 0.5515\n",
      "Epoch 15/100\n",
      "287/287 [==============================] - 3s 10ms/step - loss: 0.6887 - auc: 0.5510 - val_loss: 0.6887 - val_auc: 0.5516\n",
      "Epoch 16/100\n",
      "287/287 [==============================] - 2s 8ms/step - loss: 0.6886 - auc: 0.5516 - val_loss: 0.6887 - val_auc: 0.5510\n",
      "Epoch 17/100\n",
      "287/287 [==============================] - 2s 8ms/step - loss: 0.6885 - auc: 0.5522 - val_loss: 0.6887 - val_auc: 0.5523\n",
      "Epoch 18/100\n",
      "287/287 [==============================] - 2s 8ms/step - loss: 0.6884 - auc: 0.5524 - val_loss: 0.6886 - val_auc: 0.5526\n",
      "Epoch 19/100\n",
      "287/287 [==============================] - 3s 10ms/step - loss: 0.6883 - auc: 0.5532 - val_loss: 0.6886 - val_auc: 0.5523\n",
      "Epoch 20/100\n",
      "287/287 [==============================] - 2s 8ms/step - loss: 0.6882 - auc: 0.5533 - val_loss: 0.6889 - val_auc: 0.5501\n",
      "Epoch 21/100\n",
      "287/287 [==============================] - 2s 8ms/step - loss: 0.6881 - auc: 0.5538 - val_loss: 0.6886 - val_auc: 0.5520\n",
      "Epoch 22/100\n",
      "287/287 [==============================] - 2s 8ms/step - loss: 0.6880 - auc: 0.5544 - val_loss: 0.6886 - val_auc: 0.5530\n",
      "Epoch 23/100\n",
      "287/287 [==============================] - 2s 8ms/step - loss: 0.6880 - auc: 0.5544 - val_loss: 0.6886 - val_auc: 0.5524\n",
      "Epoch 24/100\n",
      "287/287 [==============================] - 2s 9ms/step - loss: 0.6879 - auc: 0.5553 - val_loss: 0.6886 - val_auc: 0.5519\n",
      "Epoch 25/100\n",
      "287/287 [==============================] - 2s 8ms/step - loss: 0.6878 - auc: 0.5557 - val_loss: 0.6887 - val_auc: 0.5515\n",
      "Epoch 26/100\n",
      "287/287 [==============================] - 2s 8ms/step - loss: 0.6877 - auc: 0.5558 - val_loss: 0.6886 - val_auc: 0.5526\n",
      "Epoch 27/100\n",
      "287/287 [==============================] - 2s 8ms/step - loss: 0.6876 - auc: 0.5565 - val_loss: 0.6888 - val_auc: 0.5520\n",
      "Epoch 28/100\n",
      "287/287 [==============================] - 2s 8ms/step - loss: 0.6876 - auc: 0.5563 - val_loss: 0.6888 - val_auc: 0.5518\n",
      "Epoch 29/100\n",
      "287/287 [==============================] - 2s 9ms/step - loss: 0.6874 - auc: 0.5569 - val_loss: 0.6888 - val_auc: 0.5517\n",
      "Epoch 30/100\n",
      "287/287 [==============================] - 2s 8ms/step - loss: 0.6874 - auc: 0.5570 - val_loss: 0.6887 - val_auc: 0.5521\n",
      "Epoch 31/100\n",
      "287/287 [==============================] - 2s 8ms/step - loss: 0.6872 - auc: 0.5577 - val_loss: 0.6891 - val_auc: 0.5503\n",
      "Epoch 32/100\n",
      "287/287 [==============================] - 3s 9ms/step - loss: 0.6872 - auc: 0.5580 - val_loss: 0.6889 - val_auc: 0.5508\n",
      "Epoch 1/3\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 0.6891\n",
      "Epoch 2/3\n",
      "77/77 [==============================] - 1s 11ms/step - loss: 0.6890\n",
      "Epoch 3/3\n",
      "77/77 [==============================] - 0s 6ms/step - loss: 0.6890\n",
      "Results summary\n",
      "Results in ./untitled_project\n",
      "Showing 10 best trials\n",
      "Objective(name='val_auc', direction='max')\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "init_dropout: 0.28693854858648044\n",
      "num_layers: 2\n",
      "num_units_{i}: 195\n",
      "dropout_0: 0.06895859030678211\n",
      "lr: 0.00041900125767819834\n",
      "label_smoothing: 0.06748023261965616\n",
      "dropout_1: 0.1450161123383533\n",
      "dropout_2: 0.04239853059212513\n",
      "Score: 0.5435759544372558\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "init_dropout: 0.32809722514212275\n",
      "num_layers: 3\n",
      "num_units_{i}: 202\n",
      "dropout_0: 0.11592364345264163\n",
      "lr: 0.007623403945394468\n",
      "label_smoothing: 0.07042103030668047\n",
      "dropout_1: 0.1694208985504716\n",
      "dropout_2: 0.028729039054844047\n",
      "Score: 0.5430070519447326\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "init_dropout: 0.3175041231475186\n",
      "num_layers: 2\n",
      "num_units_{i}: 206\n",
      "dropout_0: 0.12792168767443138\n",
      "lr: 0.0037750896862886224\n",
      "label_smoothing: 0.06762764213966553\n",
      "dropout_1: 0.16985096593371704\n",
      "dropout_2: 0.012535458753581507\n",
      "Score: 0.5429486036300659\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "init_dropout: 0.39412301837255853\n",
      "num_layers: 3\n",
      "num_units_{i}: 185\n",
      "dropout_0: 0.046592429057406626\n",
      "lr: 0.002889747845606823\n",
      "label_smoothing: 0.07075938577796381\n",
      "dropout_1: 0.20840012714311212\n",
      "dropout_2: 0.0\n",
      "Score: 0.5427758097648621\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "init_dropout: 0.3608685731553245\n",
      "num_layers: 3\n",
      "num_units_{i}: 189\n",
      "dropout_0: 0.07477754505739848\n",
      "lr: 0.0062189937154788775\n",
      "label_smoothing: 0.0633270545619327\n",
      "dropout_1: 0.24949917547798595\n",
      "dropout_2: 0.0005098004605795157\n",
      "Score: 0.5427438378334045\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "init_dropout: 0.3161162814542868\n",
      "num_layers: 3\n",
      "num_units_{i}: 194\n",
      "dropout_0: 0.08114163217016766\n",
      "lr: 0.007751245989156376\n",
      "label_smoothing: 0.06298818046581398\n",
      "dropout_1: 0.1673595617235369\n",
      "dropout_2: 4.357056885199438e-05\n",
      "Score: 0.5426994681358337\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "init_dropout: 0.2633846771872516\n",
      "num_layers: 2\n",
      "num_units_{i}: 217\n",
      "dropout_0: 0.06719810236327196\n",
      "lr: 0.0035395590507517082\n",
      "label_smoothing: 0.06116123651600819\n",
      "dropout_1: 0.10636404947477239\n",
      "dropout_2: 0.0\n",
      "Score: 0.5426141619682312\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "init_dropout: 0.2793242876590777\n",
      "num_layers: 3\n",
      "num_units_{i}: 168\n",
      "dropout_0: 0.12694539824668716\n",
      "lr: 0.011112863544302154\n",
      "label_smoothing: 0.0748060814640093\n",
      "dropout_1: 0.18031767447260316\n",
      "dropout_2: 0.027049431717871456\n",
      "Score: 0.5425315022468566\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "init_dropout: 0.21049666588851612\n",
      "num_layers: 2\n",
      "num_units_{i}: 240\n",
      "dropout_0: 0.12319384416089962\n",
      "lr: 0.006048793107554728\n",
      "label_smoothing: 0.09544560872939231\n",
      "dropout_1: 0.077078029065968\n",
      "dropout_2: 0.0858473781829982\n",
      "Score: 0.5423031806945801\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "init_dropout: 0.34027510975153114\n",
      "num_layers: 2\n",
      "num_units_{i}: 191\n",
      "dropout_0: 0.07231234535361568\n",
      "lr: 0.005970979199382871\n",
      "label_smoothing: 0.051795457579522676\n",
      "dropout_1: 0.1877433258597332\n",
      "dropout_2: 0.0\n",
      "Score: 0.5421371102333069\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_fn = lambda hp: create_model(hp,X.shape[-1],y.shape[-1],encoder)\n",
    "\n",
    "tuner = CVTuner(\n",
    "        hypermodel=model_fn,\n",
    "        oracle=kt.oracles.BayesianOptimization(\n",
    "        objective= kt.Objective('val_auc', direction='max'),\n",
    "        num_initial_points=4,\n",
    "        max_trials=20))\n",
    "\n",
    "FOLDS = 5\n",
    "SEED = 42\n",
    "\n",
    "if TRAINING:\n",
    "    gkf = PurgedGroupTimeSeriesSplit(n_splits = FOLDS, group_gap=20)\n",
    "    splits = list(gkf.split(y, groups=train['date'].values))\n",
    "    tuner.search((X,),(y,),splits=splits,batch_size=4096,epochs=100,callbacks=[EarlyStopping('val_auc', mode='max',patience=3)])\n",
    "    hp  = tuner.get_best_hyperparameters(1)[0]\n",
    "    pd.to_pickle(hp,f'./best_hp_{SEED}.pkl')\n",
    "    for fold, (train_indices, test_indices) in enumerate(splits):\n",
    "        model = model_fn(hp)\n",
    "        X_train, X_test = X[train_indices], X[test_indices]\n",
    "        y_train, y_test = y[train_indices], y[test_indices]\n",
    "        model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=100,batch_size=4096,callbacks=[EarlyStopping('val_auc',mode='max',patience=10,restore_best_weights=True)])\n",
    "        model.save_weights(f'./model_{SEED}_{fold}.hdf5')\n",
    "        model.compile(Adam(hp.get('lr')/100),loss='binary_crossentropy')\n",
    "        model.fit(X_test,y_test,epochs=3,batch_size=4096)\n",
    "        model.save_weights(f'./model_{SEED}_{fold}.hdf5')\n",
    "    tuner.results_summary()\n",
    "else:\n",
    "    models = []\n",
    "    hp = pd.read_pickle(f'../input/jsautoencoder/best_hp_{SEED}.pkl')\n",
    "    for f in range(FOLDS):\n",
    "        model = model_fn(hp)\n",
    "        model.load_weights(f'../input/jsautoencoder/model_{SEED}_{f}.hdf5')\n",
    "        models.append(model)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 2.030621,
     "end_time": "2020-12-18T20:16:11.081965",
     "exception": false,
     "start_time": "2020-12-18T20:16:09.051344",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-18T20:16:15.195247Z",
     "iopub.status.busy": "2020-12-18T20:16:15.194710Z",
     "iopub.status.idle": "2020-12-18T20:16:15.198455Z",
     "shell.execute_reply": "2020-12-18T20:16:15.198015Z"
    },
    "papermill": {
     "duration": 2.038446,
     "end_time": "2020-12-18T20:16:15.198588",
     "exception": false,
     "start_time": "2020-12-18T20:16:13.160142",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not TRAINING:\n",
    "    f = np.median\n",
    "    model = models[-2]\n",
    "    import janestreet\n",
    "    env = janestreet.make_env()\n",
    "    th = 0.5\n",
    "    for (test_df, pred_df) in tqdm(env.iter_test()):\n",
    "        if test_df['weight'].item() > 0:\n",
    "            x_tt = test_df.loc[:, features].values\n",
    "            if np.isnan(x_tt[:, 1:].sum()):\n",
    "                x_tt[:, 1:] = np.nan_to_num(x_tt[:, 1:]) + np.isnan(x_tt[:, 1:]) * f_mean\n",
    "            pred = model(x_tt, training = False).numpy()\n",
    "            pred = f(pred)\n",
    "            pred_df.action = np.where(pred >= th, 1, 0).astype(int)\n",
    "        else:\n",
    "            pred_df.action = 0\n",
    "        env.predict(pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 2.477994,
     "end_time": "2020-12-18T20:16:20.031743",
     "exception": false,
     "start_time": "2020-12-18T20:16:17.553749",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 5012.595413,
   "end_time": "2020-12-18T20:16:24.245016",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-12-18T18:52:51.649603",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
