{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015605,
     "end_time": "2020-12-15T01:55:40.315387",
     "exception": false,
     "start_time": "2020-12-15T01:55:40.299782",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Jane Street Market Prediction - TPU Training\n",
    "\n",
    "To run this notebook on kaggle, open the interactive editor and select to `Google Cloud SDK` from the `Addons` dropdown menu. Follow the instructions to link a Google Cloud account. Then select `TPU v3-8` as the accelerator in the `Settings` pane on the right before running the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.014427,
     "end_time": "2020-12-15T01:55:40.344554",
     "exception": false,
     "start_time": "2020-12-15T01:55:40.330127",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Run some setup code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T01:55:40.387606Z",
     "iopub.status.busy": "2020-12-15T01:55:40.382787Z",
     "iopub.status.idle": "2020-12-15T01:55:52.496633Z",
     "shell.execute_reply": "2020-12-15T01:55:52.495703Z"
    },
    "papermill": {
     "duration": 12.137843,
     "end_time": "2020-12-15T01:55:52.496785",
     "exception": false,
     "start_time": "2020-12-15T01:55:40.358942",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import AUC, PrecisionAtRecall\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "# make the x,y labels legible on plots\n",
    "plt.rc(\"axes\", labelsize=16)\n",
    "\n",
    "# set up the TPU\n",
    "tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "tf.config.experimental_connect_to_cluster(tpu)\n",
    "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "tpu_replicas = tpu_strategy.num_replicas_in_sync\n",
    "\n",
    "# get dataset credential from the Google Cloud SDK\n",
    "# and  pass credential to tensorflow\n",
    "# this needs to run after TPU intialization\n",
    "user_secrets = UserSecretsClient()\n",
    "user_credential = user_secrets.get_gcloud_credential()\n",
    "user_secrets.set_tensorflow_credential(user_credential)\n",
    "\n",
    "# set tensorflow's random seed\n",
    "tf.random.set_seed(13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013685,
     "end_time": "2020-12-15T01:55:52.525022",
     "exception": false,
     "start_time": "2020-12-15T01:55:52.511337",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T01:55:52.562330Z",
     "iopub.status.busy": "2020-12-15T01:55:52.561419Z",
     "iopub.status.idle": "2020-12-15T01:55:52.565080Z",
     "shell.execute_reply": "2020-12-15T01:55:52.564344Z"
    },
    "papermill": {
     "duration": 0.026213,
     "end_time": "2020-12-15T01:55:52.565205",
     "exception": false,
     "start_time": "2020-12-15T01:55:52.538992",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# training parameters\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 32 * tpu_replicas\n",
    "LEARNING_RATE = 0.01\n",
    "MIN_RECALL = 0.55\n",
    "LABEL_SMOOTHING = 0\n",
    "\n",
    "# cross-validation parameters\n",
    "FOLDS = 5\n",
    "HOLDOUT = 4\n",
    "\n",
    "# model parameters\n",
    "WINDOW_SIZE = 40\n",
    "NOISE = 0.05\n",
    "\n",
    "# write relevant parameters to params.json for other notebooks\n",
    "params = {\"holdout\": HOLDOUT, \"window_size\": WINDOW_SIZE}\n",
    "with open(os.path.join(os.curdir, \"params.json\"), \"w\") as file:\n",
    "    json.dump(params, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013525,
     "end_time": "2020-12-15T01:55:52.592899",
     "exception": false,
     "start_time": "2020-12-15T01:55:52.579374",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "`get_dataset()` returns a dataset generated from the folds in the list `folds`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-15T01:55:52.637166Z",
     "iopub.status.busy": "2020-12-15T01:55:52.631201Z",
     "iopub.status.idle": "2020-12-15T01:55:52.662792Z",
     "shell.execute_reply": "2020-12-15T01:55:52.663410Z"
    },
    "papermill": {
     "duration": 0.056782,
     "end_time": "2020-12-15T01:55:52.663591",
     "exception": false,
     "start_time": "2020-12-15T01:55:52.606809",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dataset(folds, repeat=False, shuffle=False, cache=False):\n",
    "    # load a dictionary mapping feature names to columns\n",
    "    col_file = os.path.join(os.pardir, \"input\",\n",
    "                            \"jane-street-market-prediction-data\",\n",
    "                            \"columns.json\")\n",
    "    with open(col_file) as file:\n",
    "        cols = json.loads(file.read())\n",
    "\n",
    "    # shorthand notation for autotune option\n",
    "    auto = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "    # opens a tf record in filename as a dataset that parses serialized\n",
    "    # tensors and returns sliding windows of WINDOW_SIZE samples\n",
    "    def open_windowed_ds(filename):\n",
    "        ds = tf.data.TFRecordDataset(filename)\n",
    "        ds = ds.map(lambda x: tf.io.parse_tensor(x, tf.float32), num_parallel_calls=auto)\n",
    "        ds = ds.window(WINDOW_SIZE, shift=1, drop_remainder=True)\n",
    "        ds = ds.flat_map(lambda x: x.batch(WINDOW_SIZE))\n",
    "        return ds\n",
    "\n",
    "    # create a dataset with filenames of tf records in files_ds\n",
    "    # then interleave the datasets obtained by calling\n",
    "    # open_windowed_ds(x) on each element of files_ds\n",
    "    data_path = KaggleDatasets().get_gcs_path()\n",
    "    patterns = [data_path + f\"/fold{fold}\" + \"/*.tfrec\" for fold in folds]\n",
    "    files = tf.io.gfile.glob(patterns)\n",
    "    files_ds = tf.data.Dataset.from_tensor_slices(files)\n",
    "    ds = files_ds.interleave(open_windowed_ds, num_parallel_calls=auto)\n",
    "\n",
    "    # filter out any time series spanning multiple dates\n",
    "    def single_date(series):\n",
    "        dates, ix = tf.unique(series[:, cols[\"date\"]])\n",
    "        return tf.equal(tf.size(dates), tf.constant(1))\n",
    "\n",
    "    ds = ds.filter(single_date)\n",
    "\n",
    "    # separate the series into a training sample consisting\n",
    "    # of the features and a label indicating whether the\n",
    "    # response at final time is positive\n",
    "    # need to explicitly reshape the tensors here for things\n",
    "    # to work properly on TPU\n",
    "    def collate(series):\n",
    "        resp = tf.reshape(series[:-1, cols[\"resp\"]], [WINDOW_SIZE - 1, 1])\n",
    "        resp = tf.pad(resp, [[0, 1], [0, 0]])\n",
    "        X = series[:, cols[\"feature_0\"]:(cols[\"feature_129\"] + 1)]\n",
    "        X = tf.concat([X, resp], 1)\n",
    "        y = (1.0 + tf.sign(series[-1, cols[\"resp\"]])) / 2.0\n",
    "        s = series[:, cols[\"ts_id\"]]   \n",
    "        return tf.reshape(X, [WINDOW_SIZE, 131]), tf.reshape(y, [1]), tf.reshape(s, [WINDOW_SIZE])\n",
    "\n",
    "    ds = ds.map(collate, num_parallel_calls=auto)\n",
    "\n",
    "    # if shuffling, allow the dataset to ignore the order for speed\n",
    "    ignore_order = tf.data.Options()\n",
    "    ignore_order.experimental_deterministic = not shuffle\n",
    "    ds = ds.with_options(ignore_order)\n",
    "\n",
    "    # check if the dataset should repeat once exhausted\n",
    "    ds = ds.repeat() if repeat else ds\n",
    "    \n",
    "    # check if we should shuffle the dataset\n",
    "    ds = ds.shuffle(4 * BATCH_SIZE) if shuffle else ds\n",
    "\n",
    "    # set the batch size of the dataset\n",
    "    ds = ds.batch(BATCH_SIZE)\n",
    "    \n",
    "    # check if we should cache the dataset\n",
    "    ds = ds.cache() if cache else ds\n",
    "\n",
    "    # prefetch new batches in the background\n",
    "    ds = ds.prefetch(auto)\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = get_dataset([0,1,2,3])\n",
    "\n",
    "ds = ds.unbatch()\n",
    "\n",
    "for X, y, s in ds.take(10):\n",
    "    print(X)\n",
    "    print(y)\n",
    "    print(s)\n",
    "    print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# location of data files\n",
    "comp_folder = os.path.join(os.pardir, \"input\", \"jane-street-market-prediction\")\n",
    "\n",
    "# load training data, convert to 32-bit floats and replace missing values by median\n",
    "train_df = pd.read_csv(os.path.join(comp_folder, \"train.csv\"))\n",
    "train_df.set_index(\"ts_id\", inplace=True)\n",
    "\n",
    "float_cols = train_df.select_dtypes(include=\"float64\").columns\n",
    "train_df = train_df.astype({c: np.float32 for c in float_cols})\n",
    "\n",
    "train_df.fillna(-100, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df[10:20])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "papermill": {
   "duration": 3053.359637,
   "end_time": "2020-12-15T02:46:27.777125",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-12-15T01:55:34.417488",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
