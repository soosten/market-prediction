{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jane Street Market Prediction - Data Conversion\n",
    "This notebook take the data from the  __[Jane Street Market Prediction](https://www.kaggle.com/c/jane-street-market-prediction)__ competition and converts it into a dataset suitable for TPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from shutil import make_archive\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# create a temporary folder to create the dataset in\n",
    "temp = os.path.join(os.pardir, \"temp\", \"tempdata\")\n",
    "os.makedirs(temp, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Settings for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of days available\n",
    "TOTAL_DAYS = 500\n",
    "\n",
    "# number of days to put in each tf record\n",
    "# one day corresponds to ~3 MB on disk\n",
    "DAYS_PER_FILE = 20\n",
    "\n",
    "# number of folds to split the data into\n",
    "FOLDS = 5\n",
    "\n",
    "# impute missing values with this value\n",
    "# EDA showed that none of the features are never very \n",
    "# negative (<= -10) so we impute missing value with -100\n",
    "NAN_VALUE = -100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the CSV file, convert values to to 32 bit floats, and impute missing values. Then store a dictionary translating column names to indices in `columns.json`. Finally, initialize the stats dictionary with an entry storing `NAN_VALUE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_folder = os.path.join(os.pardir, \"input\", \"jane-street-market-prediction\")\n",
    "df = pd.read_csv(os.path.join(comp_folder, \"train.csv\"))\n",
    "df = df.astype({c: np.float32 for c in df.select_dtypes(include=\"float64\").columns})\n",
    "df.fillna(NAN_VALUE, inplace=True)\n",
    "\n",
    "columns = {col: ix for (ix, col) in enumerate(df.columns)}\n",
    "with open(os.path.join(temp, \"columns.json\"), \"w\") as file:\n",
    "    json.dump(columns, file)\n",
    "\n",
    "stats = {\"nan_value\": NAN_VALUE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the data into `FOLDS` directories of tf records containing `DAYS_PER_FILE` days worth of data each. As we process each fold, we calculate the number of samples, mean, and variance of the data in the remaining folds and store the in the stats dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_per_fold = TOTAL_DAYS // FOLDS\n",
    "files_per_fold = TOTAL_DAYS // (FOLDS * DAYS_PER_FILE)\n",
    "\n",
    "for fold in range(FOLDS):\n",
    "    # make a directory for files in this fold\n",
    "    os.makedirs(os.path.join(temp, f\"fold{fold}\"), exist_ok=True)\n",
    "    \n",
    "    # split data into data for this fold and remainder\n",
    "    fold_cols = df[\"date\"].between(fold * days_per_fold, (fold + 1) * days_per_fold - 1)\n",
    "    fold_df, rest_df = df[fold_cols], df[~fold_cols]\n",
    "    \n",
    "    # store the statistics of the remaining data\n",
    "    stats[fold] = {\"length\": len(rest_df),\n",
    "                   \"mean\": dict(rest_df.mean()),\n",
    "                   \"variance\": dict(rest_df.var())\n",
    "                  }\n",
    "\n",
    "    # write the days for this fold into tf records\n",
    "    for file in range(files_per_fold):\n",
    "        first = fold * days_per_fold + file * DAYS_PER_FILE\n",
    "        last = first + DAYS_PER_FILE - 1\n",
    "        file_df = fold_df[fold_df[\"date\"].between(first, last)]\n",
    "                \n",
    "        # convert the corresponding part of the data frame to tensor\n",
    "        tensor = tf.convert_to_tensor(file_df, dtype=tf.float32)\n",
    "\n",
    "        # convert the tensor to a TF dataset\n",
    "        ds = tf.data.Dataset.from_tensor_slices(tensor)\n",
    "\n",
    "        # serialize the tensors in the data set\n",
    "        ds = ds.map(tf.io.serialize_tensor)\n",
    "\n",
    "        # write the serialized data to TF record\n",
    "        record_path = os.path.join(temp, f\"fold{fold}\", f\"{file}.tfrec\")\n",
    "        record = tf.data.experimental.TFRecordWriter(record_path)\n",
    "        record.write(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the stats dictionary to `stats.json` and compress the entire dataset into a zip archive in the working (output) directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(temp, \"stats.json\"), \"w\") as file:\n",
    "    json.dump(stats, file)\n",
    "\n",
    "_ = make_archive(os.path.join(os.curdir, \"data\"), \"zip\", temp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
