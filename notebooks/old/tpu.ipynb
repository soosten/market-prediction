{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.011932,
     "end_time": "2020-12-17T16:32:21.800440",
     "exception": false,
     "start_time": "2020-12-17T16:32:21.788508",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Jane Street Market Prediction - TPU Training\n",
    "\n",
    "To run this notebook on kaggle, open the interactive editor and select to `Google Cloud SDK` from the `Addons` dropdown menu. Follow the instructions to link a Google Cloud account. Then select `TPU v3-8` as the accelerator in the `Settings` pane on the right before running the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 12.521451,
     "end_time": "2020-12-17T16:32:34.355252",
     "exception": false,
     "start_time": "2020-12-17T16:32:21.833801",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import AUC, PrecisionAtRecall\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "# make the x,y labels legible on plots\n",
    "plt.rc(\"axes\", labelsize=16)\n",
    "\n",
    "# set up the TPU\n",
    "tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "tf.config.experimental_connect_to_cluster(tpu)\n",
    "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "tpu_replicas = tpu_strategy.num_replicas_in_sync\n",
    "\n",
    "# get dataset credential from the Google Cloud SDK\n",
    "# and  pass credential to tensorflow\n",
    "# this needs to run after TPU intialization\n",
    "user_secrets = UserSecretsClient()\n",
    "user_credential = user_secrets.get_gcloud_credential()\n",
    "user_secrets.set_tensorflow_credential(user_credential)\n",
    "\n",
    "# set tensorflow's random seed\n",
    "tf.random.set_seed(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.021472,
     "end_time": "2020-12-17T16:32:34.409488",
     "exception": false,
     "start_time": "2020-12-17T16:32:34.388016",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model training parameters\n",
    "BATCH_SIZE = 4096 * tpu_replicas\n",
    "LEARNING_RATE = 0.005\n",
    "EPOCHS = 200\n",
    "SHUFFLE_BUFFER = 4 * BATCH_SIZE\n",
    "LABEL_SMOOTHING = 0.01\n",
    "\n",
    "# length of the time series windows\n",
    "WINDOW_SIZE = 20\n",
    "NOISE = 0.05\n",
    "\n",
    "# show precision at this recall in metrics\n",
    "METRIC_RECALL = 0.55\n",
    "\n",
    "# cross-validation parameters\n",
    "FOLDS = 5\n",
    "HOLDOUT = 4\n",
    "TRAIN_FOLDS = [fold for fold in range(FOLDS) if fold != HOLDOUT]\n",
    "\n",
    "# write relevant parameters to params.json for other notebooks\n",
    "params = {\"holdout\": HOLDOUT, \"window_size\": WINDOW_SIZE}\n",
    "with open(os.path.join(os.curdir, \"params.json\"), \"w\") as file:\n",
    "    json.dump(params, file)\n",
    "    \n",
    "# load stats dictionary to get the number of training samples\n",
    "stats_file = os.path.join(os.pardir, \"input\",\n",
    "                          \"jane-street-market-prediction-data\",\n",
    "                          \"stats.json\")\n",
    "\n",
    "with open(stats_file) as file:\n",
    "    stats = json.loads(file.read())\n",
    "\n",
    "SAMPLES = stats[str(HOLDOUT)][\"length\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.045253,
     "end_time": "2020-12-17T16:32:34.488765",
     "exception": false,
     "start_time": "2020-12-17T16:32:34.443512",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dataset(folds, repeat=False, shuffle=False, cache=False):\n",
    "    # load a dictionary mapping feature names to columns\n",
    "    col_file = os.path.join(os.pardir, \"input\",\n",
    "                            \"jane-street-market-prediction-data\",\n",
    "                            \"columns.json\")\n",
    "    with open(col_file) as file:\n",
    "        cols = json.loads(file.read())\n",
    "\n",
    "    # shorthand notation for autotune option\n",
    "    auto = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "    # opens a tf record in filename as a dataset that parses serialized\n",
    "    # tensors and returns sliding windows of WINDOW_SIZE samples\n",
    "    def open_windowed_ds(filename):\n",
    "        ds = tf.data.TFRecordDataset(filename)\n",
    "        ds = ds.map(lambda x: tf.io.parse_tensor(x, tf.float32), num_parallel_calls=auto)\n",
    "        ds = ds.window(WINDOW_SIZE, shift=1, drop_remainder=True)\n",
    "        ds = ds.flat_map(lambda x: x.batch(WINDOW_SIZE, drop_remainder=True))\n",
    "        return ds\n",
    "\n",
    "    # create a dataset with filenames of tf records in files_ds\n",
    "    # then interleave the datasets obtained by calling\n",
    "    # open_windowed_ds(x) on each element of files_ds\n",
    "    data_path = KaggleDatasets().get_gcs_path()\n",
    "    patterns = [data_path + f\"/fold{fold}\" + \"/*.tfrec\" for fold in folds]\n",
    "    files = tf.io.gfile.glob(patterns)\n",
    "    files_ds = tf.data.Dataset.from_tensor_slices(files)\n",
    "    ds = files_ds.interleave(open_windowed_ds, num_parallel_calls=auto)\n",
    "\n",
    "    # filter out any time series spanning multiple dates\n",
    "    def single_date(series):\n",
    "        dates, ix = tf.unique(series[:, cols[\"date\"]])\n",
    "        return tf.equal(tf.size(dates), tf.constant(1))\n",
    "\n",
    "    ds = ds.filter(single_date)\n",
    "\n",
    "    # separate the series into a training sample consisting\n",
    "    # of the features and a label indicating whether the\n",
    "    # response at final time is positive\n",
    "    # need to explicitly reshape the tensors here for things\n",
    "    # to work properly on TPU\n",
    "    def collate(series):\n",
    "        X = series[:, cols[\"feature_0\"]:(cols[\"feature_129\"] + 1)]\n",
    "        y = (1.0 + tf.sign(series[-1, cols[\"resp\"]])) / 2.0\n",
    "        return tf.reshape(X, [WINDOW_SIZE, 130]), tf.reshape(y, [1])\n",
    "\n",
    "    ds = ds.map(collate, num_parallel_calls=auto)\n",
    "\n",
    "    # if shuffling, allow the dataset to ignore the order for speed\n",
    "    ignore_order = tf.data.Options()\n",
    "    ignore_order.experimental_deterministic = not shuffle\n",
    "    ds = ds.with_options(ignore_order)\n",
    "\n",
    "    # check if the dataset should repeat once exhausted\n",
    "    ds = ds.repeat() if repeat else ds\n",
    "    \n",
    "    # check if we should shuffle the dataset\n",
    "    ds = ds.shuffle(4 * BATCH_SIZE) if shuffle else ds\n",
    "\n",
    "    # set the batch size of the dataset\n",
    "    ds = ds.batch(BATCH_SIZE)\n",
    "    \n",
    "    # check if we should cache the dataset\n",
    "    ds = ds.cache() if cache else ds\n",
    "\n",
    "    # prefetch new batches in the background\n",
    "    ds = ds.prefetch(auto)\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wave block\n",
    "def Wave(inputs, filters, size, depth):\n",
    "    flow = layers.Conv1D(filters=filters,\n",
    "                         kernel_size=1, \n",
    "                         padding=\"same\",\n",
    "                         data_format=\"channels_last\")(inputs)\n",
    "    flow = layers.BatchNormalization()(flow)\n",
    "\n",
    "    \n",
    "    for d in range(depth):\n",
    "        skip = flow\n",
    "        \n",
    "        tanh = layers.Conv1D(filters=filters,\n",
    "                             kernel_size=size, \n",
    "                             padding=\"same\",\n",
    "                             dilation_rate=2 ** d,\n",
    "                             data_format=\"channels_last\")(flow)\n",
    "        tanh = layers.Activation(\"tanh\")(tanh)\n",
    "        \n",
    "        sigmoid = layers.Conv1D(filters=filters,\n",
    "                                kernel_size=size, \n",
    "                                padding=\"same\",\n",
    "                                dilation_rate=2 ** d,\n",
    "                                data_format=\"channels_last\")(flow)\n",
    "        sigmoid = layers.Activation(\"sigmoid\")(sigmoid)\n",
    "        \n",
    "        flow = layers.Multiply()([tanh, sigmoid])    \n",
    "        flow = layers.BatchNormalization()(flow)\n",
    "        \n",
    "        flow = layers.Conv1D(filters=filters,\n",
    "                             kernel_size=1,\n",
    "                             padding=\"same\",\n",
    "                             data_format=\"channels_last\")(flow)\n",
    "        \n",
    "        flow = layers.Add()([skip, flow])\n",
    "        flow = layers.BatchNormalization()(flow)\n",
    "   \n",
    "    return flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 41.194168,
     "end_time": "2020-12-17T16:33:15.718454",
     "exception": false,
     "start_time": "2020-12-17T16:32:34.524286",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# compile model on the TPU\n",
    "with tpu_strategy.scope():\n",
    "    # input & normalization\n",
    "    inputs = layers.Input(shape=[WINDOW_SIZE, 130])\n",
    "    flow = layers.BatchNormalization()(inputs)\n",
    "\n",
    "    # gaussian noise\n",
    "    flow = layers.GaussianNoise(stddev=NOISE)(flow)\n",
    "\n",
    "    # denoise/encode the features (includes normalization)\n",
    "    # this layer is trainable\n",
    "    #flow = layers.TimeDistributed(encoder)(inputs)\n",
    "\n",
    "    # wavenet with normalized dropouts of entire feature maps\n",
    "    # the first block is a bottleneck designed to strip the\n",
    "    # redundancy from the features\n",
    "    flow = Wave(flow, 64, 3, 4)\n",
    "    flow = layers.AlphaDropout(rate=0.1, noise_shape=[BATCH_SIZE, 1, 64])(flow)\n",
    "\n",
    "    flow = Wave(flow, 64, 3, 4)\n",
    "    flow = layers.AlphaDropout(rate=0.1, noise_shape=[BATCH_SIZE, 1, 128])(flow)\n",
    "\n",
    "    flow = Wave(flow, 64, 3, 4)\n",
    "    flow = layers.AlphaDropout(rate=0.1, noise_shape=[BATCH_SIZE, 1, 256])(flow)\n",
    "\n",
    "    # dense logic for final prediction\n",
    "    flow = layers.Flatten()(flow)\n",
    "    flow = layers.Dense(units=1)(flow)\n",
    "    outputs = layers.Activation(\"sigmoid\")(flow)\n",
    "\n",
    "    # optimization parameters\n",
    "    loss = BinaryCrossentropy(label_smoothing=LABEL_SMOOTHING)\n",
    "    optimizer = Adam(learning_rate=LEARNING_RATE)\n",
    "    metrics = [PrecisionAtRecall(recall=METRIC_RECALL, name=\"p@r\"), AUC(name=\"auc\")]\n",
    "\n",
    "    # compile the model and print a summary\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define callbacks for learning rate schedule and early stopping\n",
    "stopping = EarlyStopping(monitor=\"val_auc\",\n",
    "                         mode=\"max\",\n",
    "                         patience=20,\n",
    "                         min_delta=0.001,\n",
    "                         restore_best_weights=True)\n",
    "\n",
    "rate = ReduceLROnPlateau(monitor=\"val_loss\",\n",
    "                         factor=0.5,\n",
    "                         patience=5,\n",
    "                         min_lr=0.0005,\n",
    "                         min_delta=0.001)\n",
    "\n",
    "# get training and validation datasets and fit the model\n",
    "train_ds = dataset(TRAIN_FOLDS, repeat=True, shuffle=True)\n",
    "valid_ds = dataset([HOLDOUT], cache=True)\n",
    "hist = model.fit(train_ds,\n",
    "                 epochs=EPOCHS,\n",
    "                 steps_per_epoch=SAMPLES // BATCH_SIZE,\n",
    "                 validation_data=valid_ds,\n",
    "                 callbacks=[rate, stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 54.833934,
     "end_time": "2020-12-17T20:02:47.619717",
     "exception": false,
     "start_time": "2020-12-17T20:01:52.785783",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hist_df = pd.DataFrame(hist.history)\n",
    "\n",
    "# loss\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.lineplot(data=hist_df[\"loss\"], label=\"Training\")\n",
    "sns.lineplot(data=hist_df[\"val_loss\"], label=\"Validation\")\n",
    "plt.title(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"\")\n",
    "plt.show()\n",
    "\n",
    "# precision at recall\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.lineplot(data=hist_df[\"p@r\"], label=\"Training\")\n",
    "sns.lineplot(data=hist_df[\"val_p@r\"], label=\"Validation\")\n",
    "plt.title(f\"Precision at {int(100 * METRIC_RECALL)}% recall\")\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"\")\n",
    "plt.show()\n",
    "\n",
    "# area under ROC curve\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.lineplot(data=hist_df[\"auc\"], label=\"Training\")\n",
    "sns.lineplot(data=hist_df[\"val_auc\"], label=\"Validation\")\n",
    "plt.title(\"Area under the ROC curve\")\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 138.126636,
     "end_time": "2020-12-17T20:07:46.636588",
     "exception": false,
     "start_time": "2020-12-17T20:05:28.509952",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_ds = valid_ds.unbatch().map(lambda X, y: y)\n",
    "labels = np.vstack(list(y_ds.as_numpy_iterator()))\n",
    "\n",
    "X_ds = valid_ds.map(lambda X, y: X)\n",
    "probs = model.predict(X_ds)\n",
    "\n",
    "# precision vs recall\n",
    "precisions, recalls, thresholds = precision_recall_curve(labels, probs)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(thresholds, precisions[:-1], \"tab:blue\", label=\"Precision\")\n",
    "plt.plot(thresholds, recalls[:-1], \"tab:orange\", label=\"Recall\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.title(\"Precision/recall at threshold\")\n",
    "plt.axis([0, 1, 0, 1])\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(recalls, precisions, \"tab:blue\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision at recall\")\n",
    "plt.axis([0, 1, 0, 1])\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# ROC curve\n",
    "false_positives, true_positives, thresholds = roc_curve(labels, probs)\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(false_positives, true_positives, \"tab:blue\")\n",
    "plt.plot([0, 1], [0, 1], \"tab:gray\")\n",
    "plt.xlabel(\"False positive rate\")\n",
    "plt.ylabel(\"True positive rate\")\n",
    "plt.title(\"ROC curve\")\n",
    "plt.axis([0, 1, 0, 1])\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 54.000473,
     "end_time": "2020-12-17T20:11:22.996408",
     "exception": false,
     "start_time": "2020-12-17T20:10:28.995935",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"model.json\", \"w\") as f:\n",
    "    f.write(model.to_json())\n",
    "\n",
    "model.save_weights(\"model.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "papermill": {
   "duration": 13201.715307,
   "end_time": "2020-12-17T20:12:18.231424",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-12-17T16:32:16.516117",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
