{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.008915,
     "end_time": "2020-12-26T23:34:09.435916",
     "exception": false,
     "start_time": "2020-12-26T23:34:09.427001",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Jane Street - TPU\n",
    "To run this notebook on kaggle, open the interactive editor and select to `Google Cloud SDK` from the `Addons` dropdown menu. Follow the instructions to link a Google Cloud account. Then select `TPU v3-8` as the accelerator in the `Settings` pane on the right before running the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 11.787822,
     "end_time": "2020-12-26T23:34:21.231308",
     "exception": false,
     "start_time": "2020-12-26T23:34:09.443486",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler, ReduceLROnPlateau\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import AUC, PrecisionAtRecall\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "# make the x,y labels legible on plots\n",
    "plt.rc(\"axes\", labelsize=16)\n",
    "\n",
    "# set up the TPU\n",
    "tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "tf.config.experimental_connect_to_cluster(tpu)\n",
    "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "tpu_replicas = tpu_strategy.num_replicas_in_sync\n",
    "\n",
    "# get dataset credential from the Google Cloud SDK\n",
    "# and  pass credential to tensorflow\n",
    "# this needs to run after TPU intialization\n",
    "user_secrets = UserSecretsClient()\n",
    "user_credential = user_secrets.get_gcloud_credential()\n",
    "user_secrets.set_tensorflow_credential(user_credential)\n",
    "\n",
    "# set tensorflow's random seed\n",
    "tf.random.set_seed(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.034266,
     "end_time": "2020-12-26T23:34:21.273216",
     "exception": false,
     "start_time": "2020-12-26T23:34:21.238950",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model training parameters\n",
    "BATCH_SIZE = 4096 * tpu_replicas\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 200\n",
    "SHUFFLE_BUFFER = 4 * BATCH_SIZE\n",
    "\n",
    "# length of the time series windows\n",
    "WINDOW_SIZE = 20\n",
    "NOISE = 0.05\n",
    "\n",
    "# show precision at this recall in metrics\n",
    "METRIC_RECALL = 0.55\n",
    "\n",
    "# cross-validation parameters\n",
    "FOLDS = 5\n",
    "HOLDOUT = 4\n",
    "TRAIN_FOLDS = [fold for fold in range(FOLDS) if fold != HOLDOUT]\n",
    "\n",
    "# write relevant parameters to params.json for other notebooks\n",
    "params = {\"holdout\": HOLDOUT, \"window_size\": WINDOW_SIZE}\n",
    "with open(os.path.join(os.curdir, \"params.json\"), \"w\") as file:\n",
    "    json.dump(params, file)\n",
    "    \n",
    "# load stats dictionary to get statistics of training samples\n",
    "stats_file = os.path.join(os.pardir, \"input\", \"jane-street-data\", \"stats.json\")\n",
    "\n",
    "with open(stats_file) as file:\n",
    "    stats = json.loads(file.read())\n",
    "\n",
    "SAMPLES = stats[str(HOLDOUT)][\"samples\"]\n",
    "MEAN = float(stats[str(HOLDOUT)][\"mean\"])\n",
    "STD = float(stats[str(HOLDOUT)][\"std\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.032105,
     "end_time": "2020-12-26T23:34:21.312876",
     "exception": false,
     "start_time": "2020-12-26T23:34:21.280771",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dataset(folds, repeat=False, shuffle=False, cache=False):\n",
    "    # load a dictionary mapping feature names to columns\n",
    "    col_file = os.path.join(os.pardir, \"input\", \"jane-street-data\", \"columns.json\")\n",
    "    with open(col_file) as file:\n",
    "        cols = json.loads(file.read())\n",
    "\n",
    "    # shorthand notation for autotune option\n",
    "    auto = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "    # opens a tf record in filename as a dataset that parses serialized\n",
    "    # tensors and returns sliding windows of WINDOW_SIZE samples\n",
    "    def open_windowed_ds(filename):\n",
    "        ds = tf.data.TFRecordDataset(filename)\n",
    "        ds = ds.map(lambda x: tf.io.parse_tensor(x, tf.float32), num_parallel_calls=auto)\n",
    "        ds = ds.window(WINDOW_SIZE, shift=1, drop_remainder=True)\n",
    "        ds = ds.flat_map(lambda x: x.batch(WINDOW_SIZE))\n",
    "        return ds\n",
    "\n",
    "    # create a dataset with filenames of tf records in files_ds\n",
    "    # then interleave the datasets obtained by calling\n",
    "    # open_windowed_ds(x) on each element of files_ds\n",
    "    data_path = KaggleDatasets().get_gcs_path()\n",
    "    patterns = [data_path + f\"/fold{fold}\" + \"/*.tfrec\" for fold in folds]\n",
    "    files = tf.io.gfile.glob(patterns)\n",
    "    files_ds = tf.data.Dataset.from_tensor_slices(files)\n",
    "    ds = files_ds.interleave(open_windowed_ds, num_parallel_calls=auto)\n",
    "\n",
    "    # filter out any time series spanning multiple dates\n",
    "    def single_date(series):\n",
    "        dates, ix = tf.unique(series[:, cols[\"date\"]])\n",
    "        return tf.equal(tf.size(dates), tf.constant(1))\n",
    "\n",
    "    ds = ds.filter(single_date)\n",
    "\n",
    "    # separate the series into a training sample consisting\n",
    "    # of the features and a label indicating whether the\n",
    "    # response at final time is positive\n",
    "    # need to explicitly reshape the tensors here for things\n",
    "    # to work properly on TPU\n",
    "    def collate(series):\n",
    "        X = series[:, cols[\"feature_0\"]:(cols[\"feature_129\"] + 1)]\n",
    "        y = (1.0 + tf.sign(series[-1, cols[\"resp\"]])) / 2.0\n",
    "        return tf.reshape(X, [WINDOW_SIZE, 130]), tf.reshape(y, [1])\n",
    "\n",
    "    ds = ds.map(collate, num_parallel_calls=auto)\n",
    "\n",
    "    # if shuffling, allow the dataset to ignore the order for speed\n",
    "    ignore_order = tf.data.Options()\n",
    "    ignore_order.experimental_deterministic = not shuffle\n",
    "    ds = ds.with_options(ignore_order)\n",
    "    \n",
    "    # check if we should cache the dataset\n",
    "    ds = ds.cache() if cache else ds\n",
    "    \n",
    "    # check if we should shuffle the dataset\n",
    "    ds = ds.shuffle(SHUFFLE_BUFFER) if shuffle else ds\n",
    "\n",
    "    # check if the dataset should repeat once exhausted\n",
    "    ds = ds.repeat() if repeat else ds\n",
    "    \n",
    "    # set the batch size of the dataset\n",
    "    ds = ds.batch(BATCH_SIZE)\n",
    "\n",
    "    # prefetch new batches in the background\n",
    "    ds = ds.prefetch(auto)\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 2.19904,
     "end_time": "2020-12-26T23:34:23.519636",
     "exception": false,
     "start_time": "2020-12-26T23:34:21.320596",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# compile model on the TPU\n",
    "with tpu_strategy.scope():\n",
    "    # input & normalization\n",
    "    inputs = layers.Input(shape=[WINDOW_SIZE, 130])\n",
    "    flow = layers.BatchNormalization()(inputs)\n",
    "    \n",
    "    flow = layers.GaussianNoise(stddev=0.2)(flow)\n",
    "    \n",
    "    # flattened dense logic\n",
    "    for units in [40, 40, 40, 40]:\n",
    "        flow = layers.Conv1D(filters=units, kernel_size=3, padding=\"valid\")(flow)\n",
    "        flow = layers.Activation(keras.activations.swish)(flow)\n",
    "        flow = layers.SpatialDropout1D(rate=0.3)(flow)\n",
    "    \n",
    "    flow = layers.Flatten()(flow)\n",
    "    outputs = layers.Dense(units=1, activation=\"sigmoid\")(flow)\n",
    "    \n",
    "    # optimization parameters\n",
    "    loss = \"binary_crossentropy\"\n",
    "    optimizer = Adam(learning_rate=LEARNING_RATE) \n",
    "    metrics = [PrecisionAtRecall(recall=METRIC_RECALL, name=\"p@r\"), AUC(name=\"auc\")]\n",
    "    \n",
    "    # compile the model and print a summary\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 859.514305,
     "end_time": "2020-12-26T23:48:43.042619",
     "exception": false,
     "start_time": "2020-12-26T23:34:23.528314",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define callbacks for learning rate schedule and early stopping\n",
    "stopping = EarlyStopping(monitor=\"val_auc\",\n",
    "                         mode=\"max\",\n",
    "                         patience=20,\n",
    "                         min_delta=0.001,\n",
    "                         restore_best_weights=True)\n",
    "\n",
    "rate = ReduceLROnPlateau(monitor=\"val_auc\",\n",
    "                         mode=\"max\",\n",
    "                         factor=0.5,\n",
    "                         patience=15,\n",
    "                         min_lr=0.0001,\n",
    "                         min_delta=0.001)\n",
    "\n",
    "# def schedule(epoch, lr):\n",
    "#     return lr\n",
    "\n",
    "# rate = LearningRateScheduler(schedule)\n",
    "\n",
    "# get training and validation datasets and fit the model\n",
    "train_ds = dataset(TRAIN_FOLDS, cache=True, repeat=True, shuffle=True)\n",
    "valid_ds = dataset([HOLDOUT], cache=True)\n",
    "hist = model.fit(train_ds,\n",
    "                 epochs=EPOCHS,\n",
    "                 steps_per_epoch=SAMPLES // BATCH_SIZE,\n",
    "                 validation_data=valid_ds,\n",
    "                 callbacks=[rate, stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 1.996487,
     "end_time": "2020-12-26T23:48:46.149593",
     "exception": false,
     "start_time": "2020-12-26T23:48:44.153106",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hist_df = pd.DataFrame(hist.history)\n",
    "\n",
    "# loss\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.lineplot(data=hist_df[\"loss\"], label=\"Training\")\n",
    "sns.lineplot(data=hist_df[\"val_loss\"], label=\"Validation\")\n",
    "plt.title(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"\")\n",
    "plt.show()\n",
    "\n",
    "# precision at recall\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.lineplot(data=hist_df[\"p@r\"], label=\"Training\")\n",
    "sns.lineplot(data=hist_df[\"val_p@r\"], label=\"Validation\")\n",
    "plt.title(f\"Precision at {int(100 * METRIC_RECALL)}% recall\")\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"\")\n",
    "plt.show()\n",
    "\n",
    "# area under ROC curve\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.lineplot(data=hist_df[\"auc\"], label=\"Training\")\n",
    "sns.lineplot(data=hist_df[\"val_auc\"], label=\"Validation\")\n",
    "plt.title(\"Area under the ROC curve\")\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 1412.328818,
     "end_time": "2020-12-27T00:12:19.561891",
     "exception": false,
     "start_time": "2020-12-26T23:48:47.233073",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_ds = valid_ds.unbatch().map(lambda X, y: y)\n",
    "labels = np.vstack(list(y_ds.as_numpy_iterator()))\n",
    "\n",
    "X_ds = valid_ds.map(lambda X, y: X)\n",
    "probs = model.predict(X_ds)\n",
    "\n",
    "# precision vs recall\n",
    "precisions, recalls, thresholds = precision_recall_curve(labels, probs)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(thresholds, precisions[:-1], \"tab:blue\", label=\"Precision\")\n",
    "plt.plot(thresholds, recalls[:-1], \"tab:orange\", label=\"Recall\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.title(\"Precision/recall at threshold\")\n",
    "plt.axis([0, 1, 0, 1])\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(recalls, precisions, \"tab:blue\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision at recall\")\n",
    "plt.axis([0, 1, 0, 1])\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# ROC curve\n",
    "false_positives, true_positives, thresholds = roc_curve(labels, probs)\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(false_positives, true_positives, \"tab:blue\")\n",
    "plt.plot([0, 1], [0, 1], \"tab:gray\")\n",
    "plt.xlabel(\"False positive rate\")\n",
    "plt.ylabel(\"True positive rate\")\n",
    "plt.title(\"ROC curve\")\n",
    "plt.axis([0, 1, 0, 1])\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 1.189853,
     "end_time": "2020-12-27T00:12:21.820741",
     "exception": false,
     "start_time": "2020-12-27T00:12:20.630888",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"model.json\", \"w\") as f:\n",
    "    f.write(model.to_json())\n",
    "\n",
    "model.save_weights(\"model.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "papermill": {
   "duration": 2298.536012,
   "end_time": "2020-12-27T00:12:23.043640",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-12-26T23:34:04.507628",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
