{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jane Street Market Prediction - TPU Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run some setup code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 24.015539,
     "end_time": "2020-10-18T23:33:34.951606",
     "exception": false,
     "start_time": "2020-10-18T23:33:10.936067",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "# set up the TPU\n",
    "tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "tf.config.experimental_connect_to_cluster(tpu)\n",
    "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "tpu_replicas = tpu_strategy.num_replicas_in_sync\n",
    "\n",
    "# get dataset credential from the Google Cloud SDK\n",
    "# and  pass credential to tensorflow\n",
    "# this needs to run after TPU intialization\n",
    "user_secrets = UserSecretsClient()\n",
    "user_credential = user_secrets.get_gcloud_credential()\n",
    "user_secrets.set_tensorflow_credential(user_credential)\n",
    "\n",
    "# set tensorflow's random seed\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 512 * tpu_replicas\n",
    "LEARNING_RATE = 0.2\n",
    "\n",
    "# cross-validation parameters\n",
    "FOLDS = 5\n",
    "HOLDOUT = 4\n",
    "\n",
    "# model parameters\n",
    "WINDOW_SIZE = 20\n",
    "NOISE = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_dataset()` returns a dataset generated from the folds in the list `folds`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 1.588044,
     "end_time": "2020-10-18T23:33:50.745129",
     "exception": false,
     "start_time": "2020-10-18T23:33:49.157085",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dataset(folds, repeat=False, shuffle=False, cache=False):\n",
    "    # load a dictionary mapping feature names to columns\n",
    "    col_file = os.path.join(os.pardir, \"input\",\n",
    "                            \"jane-street-market-prediction-data\",\n",
    "                            \"columns.json\")\n",
    "    with open(col_file) as file:\n",
    "        cols = json.loads(file.read())\n",
    "\n",
    "    # shorthand notation for autotune option\n",
    "    auto = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "    # opens a tf record in filename as a dataset that parses serialized\n",
    "    # tensors and returns sliding windows of WINDOW_SIZE samples\n",
    "    def open_windowed_ds(filename):\n",
    "        ds = tf.data.TFRecordDataset(filename)\n",
    "        ds = ds.map(lambda x: tf.io.parse_tensor(x, tf.float32), num_parallel_calls=auto)\n",
    "        ds = ds.window(WINDOW_SIZE, shift=1, drop_remainder=True)\n",
    "        ds = ds.flat_map(lambda x: x.batch(WINDOW_SIZE))\n",
    "        return ds\n",
    "\n",
    "    # create a dataset with filenames of tf records in files_ds\n",
    "    # then interleave the datasets obtained by calling\n",
    "    # open_windowed_ds(x) on each element of files_ds\n",
    "    data_path = KaggleDatasets().get_gcs_path()\n",
    "    patterns = [data_path + f\"/fold{fold}\" + \"/*.tfrec\" for fold in folds]\n",
    "    files = tf.io.gfile.glob(patterns)\n",
    "    files_ds = tf.data.Dataset.from_tensor_slices(files)\n",
    "    ds = files_ds.interleave(open_windowed_ds, num_parallel_calls=auto)\n",
    "\n",
    "    # filter out any time series spanning multiple dates\n",
    "    def single_date(series):\n",
    "        dates, ix = tf.unique(series[:, cols[\"date\"]])\n",
    "        return tf.equal(tf.size(dates), tf.constant(1))\n",
    "\n",
    "    ds = ds.filter(single_date)\n",
    "\n",
    "    # separate the series into a training sample consisting\n",
    "    # of the features and a label indicating whether the\n",
    "    # response at final time is positive\n",
    "    # need to explicitly reshape the tensors here for things\n",
    "    # to work properly on TPU\n",
    "    def collate(series):\n",
    "        X = series[:, cols[\"feature_0\"]:(cols[\"feature_129\"] + 1)]\n",
    "        y = (1.0 + tf.sign(series[-1, cols[\"resp\"]])) / 2.0\n",
    "        return tf.reshape(X, [WINDOW_SIZE, 130]), tf.reshape(y, [1])\n",
    "    \n",
    "    ds = ds.map(collate, num_parallel_calls=auto)\n",
    "\n",
    "    # if shuffling, allow the dataset to ignore the order for speed\n",
    "    ignore_order = tf.data.Options()\n",
    "    ignore_order.experimental_deterministic = not shuffle\n",
    "    ds = ds.with_options(ignore_order)\n",
    "\n",
    "    # check if the dataset should repeat once exhausted\n",
    "    if repeat:\n",
    "        ds = ds.repeat()\n",
    "    \n",
    "    # check if we should shuffle the dataset\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(4 * BATCH_SIZE)\n",
    "\n",
    "    # set the batch size of the dataset\n",
    "    ds = ds.batch(BATCH_SIZE)\n",
    "    \n",
    "    # check if we should cache the dataset\n",
    "    if cache:\n",
    "        ds = ds.cache()\n",
    "\n",
    "    # prefetch new batches in the background\n",
    "    ds = ds.prefetch(auto)\n",
    "\n",
    "    return ds\n",
    "\n",
    "\n",
    "# load a simpler non-windowed version of training set\n",
    "# to adapt the normalization layer - see comments in\n",
    "# get_dataset() for explanations\n",
    "def get_norm_dataset(folds):\n",
    "    col_file = os.path.join(os.pardir, \"input\",\n",
    "                            \"jane-street-market-prediction-data\",\n",
    "                            \"columns.json\")\n",
    "\n",
    "    with open(col_file) as file:\n",
    "        cols = json.loads(file.read())\n",
    "\n",
    "    def parse(serialized):\n",
    "        tensor = tf.io.parse_tensor(serialized, tf.float32)\n",
    "        tensor = tensor[cols[\"feature_0\"]:(cols[\"feature_129\"] + 1)]\n",
    "        return tf.reshape(tensor, [130])\n",
    "\n",
    "    auto = tf.data.experimental.AUTOTUNE\n",
    "    ignore_order = tf.data.Options()\n",
    "    ignore_order.experimental_deterministic = False\n",
    "\n",
    "    data_path = KaggleDatasets().get_gcs_path()\n",
    "    patterns = [data_path + f\"/fold{fold}\" + \"/*.tfrec\" for fold in folds]\n",
    "    ds = tf.data.TFRecordDataset(tf.io.gfile.glob(patterns), num_parallel_reads=auto)\n",
    "    \n",
    "    ds = ds.with_options(ignore_order)\n",
    "    ds = ds.map(parse, num_parallel_calls=auto)\n",
    "    ds = ds.batch(BATCH_SIZE)\n",
    "    ds = ds.prefetch(auto)\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folds = [fold for fold in range(FOLDS) if fold != HOLDOUT]\n",
    "\n",
    "# compile model on the TPU\n",
    "with tpu_strategy.scope():    \n",
    "    # input layer\n",
    "    inputs = layers.Input(shape=[WINDOW_SIZE, 130])\n",
    "    \n",
    "    # normalization layer\n",
    "    norm = preprocessing.Normalization()\n",
    "    norm_ds = get_norm_dataset(train_folds)\n",
    "    norm.adapt(norm_ds)\n",
    "    flow = norm(inputs)\n",
    "    \n",
    "    # gaussian noise\n",
    "    flow = layers.GaussianNoise(stddev=NOISE)(flow)\n",
    "    \n",
    "    # minimalistic resnet\n",
    "    # skip connection\n",
    "    skip = flow\n",
    "    skip = layers.Conv1D(filters=64,\n",
    "                         kernel_size=5,\n",
    "                         activation=None,\n",
    "                         padding=\"same\",\n",
    "                         data_format=\"channels_last\")(skip)\n",
    "    skip = layers.BatchNormalization()(skip)\n",
    "    \n",
    "    # convolutional block\n",
    "    flow = layers.Conv1D(filters=64,\n",
    "                         kernel_size=5,\n",
    "                         activation=None,\n",
    "                         padding=\"same\",\n",
    "                         data_format=\"channels_last\")(flow)\n",
    "    flow = layers.BatchNormalization()(flow)\n",
    "    flow = layers.Activation(activation=\"relu\")(flow)\n",
    "\n",
    "    flow = layers.Conv1D(filters=64,\n",
    "                         kernel_size=5,\n",
    "                         activation=None,\n",
    "                         padding=\"same\",\n",
    "                         data_format=\"channels_last\")(flow)\n",
    "    flow = layers.BatchNormalization()(flow)\n",
    "\n",
    "    # merge flow and skip\n",
    "    flow = layers.Add()([skip, flow])\n",
    "    flow = layers.Activation(activation=\"relu\")(flow)\n",
    "    \n",
    "    # flattened layers and dense logic for prediction\n",
    "    flow = layers.Flatten()(flow)\n",
    "    outputs = layers.Dense(units=1, activation=\"sigmoid\")(flow)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"model\")\n",
    "        \n",
    "    # binary cross-entropy as loss function, Nexterov SGD as optimizer,\n",
    "    # track precision, recall, and are under the ROC curve\n",
    "    optimizer = keras.optimizers.SGD(learning_rate=LEARNING_RATE,\n",
    "                                     momentum=0.9,\n",
    "                                     nesterov=True)\n",
    "    metrics=[keras.metrics.Recall(name=\"recall\"),\n",
    "             keras.metrics.Precision(name=\"precision\"),\n",
    "             keras.metrics.AUC(name=\"auc\")]\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(name=\"binary_crossentropy\")\n",
    "\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "    \n",
    "    # print a summary of the model\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the training and validation datasets\n",
    "train_ds = get_dataset(train_folds, repeat=True, shuffle=True)\n",
    "valid_ds = get_dataset([HOLDOUT], cache=True)\n",
    "\n",
    "# load stats dictionary to get the number of training samples\n",
    "stats_file = os.path.join(os.pardir, \"input\",\n",
    "                          \"jane-street-market-prediction-data\",\n",
    "                          \"stats.json\")\n",
    "\n",
    "with open(stats_file) as file:\n",
    "    stats = json.loads(file.read())\n",
    "\n",
    "# train the model\n",
    "hist = model.fit(train_ds,\n",
    "                 epochs=EPOCHS,\n",
    "                 steps_per_epoch=stats[str(HOLDOUT)][\"length\"] // BATCH_SIZE,\n",
    "                 validation_data=valid_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the learning curve and associated metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_df = pd.DataFrame(hist.history)\n",
    "\n",
    "# loss\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.lineplot(data=hist_df[\"loss\"], label=\"Training loss\")\n",
    "sns.lineplot(data=hist_df[\"val_loss\"], label=\"Validation loss\")\n",
    "plt.title(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"\")\n",
    "plt.show()\n",
    "\n",
    "# training metrics\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.lineplot(data=hist_df[\"precision\"], label=\"Precision\")\n",
    "sns.lineplot(data=hist_df[\"recall\"], label=\"Recall\")\n",
    "sns.lineplot(data=hist_df[\"auc\"], label=\"Area under ROC curve\")\n",
    "plt.title(\"Training metrics\")\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"\")\n",
    "plt.show()\n",
    "\n",
    "# validation metrics\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.lineplot(data=hist_df[\"val_precision\"], label=\"Precision\")\n",
    "sns.lineplot(data=hist_df[\"val_recall\"], label=\"Recall\")\n",
    "sns.lineplot(data=hist_df[\"val_auc\"], label=\"Area under ROC curve\")\n",
    "plt.title(\"Validation metrics\")\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot ROC curve and precision vs. recall on validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = get_dataset([HOLDOUT])\n",
    "\n",
    "y_ds = test_ds.map(lambda X, y: y)\n",
    "labels = np.vstack(list(y_ds.as_numpy_iterator()))\n",
    "\n",
    "X_ds = test_ds.map(lambda X, y: X)\n",
    "probs = model.predict(X_ds)\n",
    "\n",
    "# precision vs recall\n",
    "precisions, recalls, thresholds = precision_recall_curve(labels, probs)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\", linewidth=2)\n",
    "plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\", linewidth=2)\n",
    "plt.legend()\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.title(\"Precision/recall at threshold\")\n",
    "plt.axis([0, 1, 0, 1])\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(recalls, precisions, \"b-\", linewidth=2)\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision at recall\")\n",
    "plt.axis([0, 1, 0, 1])\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(labels, probs)\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(fpr, tpr, linewidth=2)\n",
    "plt.xlabel(\"False positive rate\")\n",
    "plt.ylabel(\"True positive rate\")\n",
    "plt.title(\"ROC curve\")\n",
    "plt.axis([0, 1, 0, 1])\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model architecture and weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model.json\", \"w\") as f:\n",
    "    f.write(model.to_json())\n",
    "\n",
    "model.save_weights(\"model.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "papermill": {
   "duration": 5526.40816,
   "end_time": "2020-10-19T01:05:12.285603",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-10-18T23:33:05.877443",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
