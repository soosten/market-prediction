{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jane Street Market Prediction - TPU Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run some setup code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 24.015539,
     "end_time": "2020-10-18T23:33:34.951606",
     "exception": false,
     "start_time": "2020-10-18T23:33:10.936067",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "# set up the TPU\n",
    "tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "tf.config.experimental_connect_to_cluster(tpu)\n",
    "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "tpu_replicas = tpu_strategy.num_replicas_in_sync\n",
    "\n",
    "# get dataset credential from the Google Cloud SDK\n",
    "# and  pass credential to tensorflow\n",
    "# this needs to run after TPU intialization\n",
    "user_secrets = UserSecretsClient()\n",
    "user_credential = user_secrets.get_gcloud_credential()\n",
    "user_secrets.set_tensorflow_credential(user_credential)\n",
    "\n",
    "# set tensorflow's random seed\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 512 * tpu_replicas\n",
    "\n",
    "# cross-validation parameters\n",
    "FOLDS = 5\n",
    "HOLDOUT = 4\n",
    "\n",
    "# model parameters\n",
    "WINDOW_SIZE = 20\n",
    "NOISE = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_dataset()` returns a dataset generated from the folds in the list `folds`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 1.588044,
     "end_time": "2020-10-18T23:33:50.745129",
     "exception": false,
     "start_time": "2020-10-18T23:33:49.157085",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dataset(folds, repeat=False, shuffle=False, cache=False):\n",
    "    # load a dictionary mapping feature names to columns\n",
    "    col_file = os.path.join(os.pardir, \"input\",\n",
    "                            \"jane-street-market-prediction-data\",\n",
    "                            \"columns.json\")\n",
    "    with open(col_file) as file:\n",
    "        cols = json.loads(file.read())\n",
    "\n",
    "    # shorthand notation for autotune option\n",
    "    auto = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "    # opens a tf record in filename as a dataset that parses serialized\n",
    "    # tensors and returns sliding windows of WINDOW_SIZE samples\n",
    "    def open_windowed_ds(filename):\n",
    "        ds = tf.data.TFRecordDataset(filename)\n",
    "        ds = ds.map(lambda x: tf.io.parse_tensor(x, tf.float32), num_parallel_calls=auto) # !!!!\n",
    "        ds = ds.window(WINDOW_SIZE, shift=1, drop_remainder=True)\n",
    "        ds = ds.flat_map(lambda x: x.batch(WINDOW_SIZE))\n",
    "        return ds\n",
    "\n",
    "    # create a dataset with filenames of tf records in files_ds\n",
    "    # then interleave the datasets obtained by calling\n",
    "    # open_windowed_ds(x) on each element of files_ds\n",
    "    data_path = KaggleDatasets().get_gcs_path()\n",
    "    patterns = [data_path + f\"/fold{fold}\" + \"/*.tfrec\" for fold in folds]\n",
    "    files = tf.io.gfile.glob(patterns)\n",
    "    files_ds = tf.data.Dataset.from_tensor_slices(files)\n",
    "    ds = files_ds.interleave(open_windowed_ds, num_parallel_calls=auto)\n",
    "\n",
    "    # filter out any time series spanning multiple dates\n",
    "    def single_date(series):\n",
    "        dates, ix = tf.unique(series[:, cols[\"date\"]])\n",
    "        return tf.equal(tf.size(dates), tf.constant(1))\n",
    "\n",
    "    ds = ds.filter(single_date)\n",
    "\n",
    "    # separate the series into a training sample consisting\n",
    "    # of the features and a label indicating whether the\n",
    "    # response at final time is positive\n",
    "    # need to explicitly reshape the tensors here for things\n",
    "    # to work properly on TPU\n",
    "    def collate(series):\n",
    "        X = series[:, cols[\"feature_0\"]:(cols[\"feature_129\"] + 1)]\n",
    "        y = (1.0 + tf.sign(series[-1, cols[\"resp\"]])) / 2.0\n",
    "        return tf.reshape(X, [WINDOW_SIZE, 130]), tf.reshape(y, [1])\n",
    "    \n",
    "    ds = ds.map(collate, num_parallel_calls=auto)\n",
    "\n",
    "    # allow the dataset to ignore the order for speed\n",
    "    ignore_order = tf.data.Options()\n",
    "    ignore_order.experimental_deterministic = False\n",
    "    ds = ds.with_options(ignore_order)\n",
    "\n",
    "    # check if the dataset should repeat once exhausted\n",
    "    if repeat:\n",
    "        ds = ds.repeat()\n",
    "    \n",
    "    # check if we should shuffle the dataset\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(16 * BATCH_SIZE)\n",
    "\n",
    "    # set the batch size of the dataset\n",
    "    ds = ds.batch(BATCH_SIZE)\n",
    "    \n",
    "    # check if we should cache the dataset\n",
    "    if cache:\n",
    "        ds = ds.cache()\n",
    "\n",
    "    # prefetch new batches in the background\n",
    "    ds = ds.prefetch(auto)\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get features from training dataset for the normalization layer\n",
    "train_folds = [fold for fold in range(FOLDS) if fold != HOLDOUT]\n",
    "norm_ds = get_dataset(train_folds)\n",
    "norm_ds = norm_ds.map(lambda x, y: x)\n",
    "\n",
    "# compile model on the TPU\n",
    "with tpu_strategy.scope():\n",
    "    \n",
    "    # normalization layer\n",
    "    # gaussian noise (is this first or second!?) (strong, probably...)\n",
    "    # conv1d (seperable conv1d? )\n",
    "    # max pooling\n",
    "    \n",
    "    # experiment with Gaussian dropout - its multiplicative like a stock?\n",
    "    \n",
    "    # wave net type of architecture...\n",
    "    # what kind of regularization in there?\n",
    "    \n",
    "    # norm -> noise -> batch norm?\n",
    "    # use tanh activation?\n",
    "    \n",
    "    # input layer\n",
    "    inputs = layers.Input(shape=[WINDOW_SIZE, 130], name=\"inputs\")\n",
    "    \n",
    "    # normalization layer\n",
    "    norm = preprocessing.Normalization(name=\"normalization\")\n",
    "    norm.adapt(norm_ds)\n",
    "    flow = norm(inputs)\n",
    "    \n",
    "    # gaussian noise\n",
    "    flow = layers.GaussianNoise(NOISE, name=\"noise\")(flow)\n",
    "    \n",
    "    # convolutional net (for now)\n",
    "    flow = layers.Conv1D(filters=256,\n",
    "                         kernel_size=7,\n",
    "                         activation=\"relu\",\n",
    "                         padding=\"same\",\n",
    "                         data_format=\"channels_last\")(flow)\n",
    "    \n",
    "    flow = layers.MaxPooling1D(2)(flow)\n",
    "    \n",
    "    flow = layers.Conv1D(filters=256,\n",
    "                         kernel_size=3,\n",
    "                         activation=\"relu\",\n",
    "                         padding=\"same\",\n",
    "                         data_format=\"channels_last\")(flow)\n",
    "\n",
    "    flow = layers.Conv1D(filters=256,\n",
    "                         kernel_size=3,\n",
    "                         activation=\"relu\",\n",
    "                         padding=\"same\",\n",
    "                         data_format=\"channels_last\")(flow)\n",
    "    \n",
    "    flow = layers.MaxPooling1D(2)(flow)\n",
    "    \n",
    "    # flattened layers and dense logic towards the end\n",
    "    flow = layers.Flatten()(flow)\n",
    "    \n",
    "    flow = layers.Dense(128, activation=\"relu\")(flow)\n",
    "    flow = layers.Dropout(0.5)(flow)\n",
    "\n",
    "    flow = layers.Dense(64, activation=\"relu\")(flow)\n",
    "    flow = layers.Dropout(0.5)(flow)\n",
    "    \n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(flow)\n",
    "    \n",
    "    \n",
    "    # apply sigmoid activation to get outputs between 0 and 1\n",
    "    # outputs = layers.Activation(\"sigmoid\", name=\"outputs\")(flow)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs, name=\"model\")\n",
    "        \n",
    "    # binary cross-entropy as loss function, ADAM as optimizer,\n",
    "    # track precision, recall, and are under the ROC curve\n",
    "    metrics=[keras.metrics.Recall(), keras.metrics.Precision(), keras.metrics.AUC()]\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=metrics)\n",
    "    \n",
    "    # print a summary of the model\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the training and validation datasets\n",
    "train_ds = get_dataset(train_folds, repeat=True, shuffle=True)\n",
    "valid_ds = get_dataset([HOLDOUT], cache=True)\n",
    "\n",
    "# load stats dictionary to get the number of training samples\n",
    "stats_file = os.path.join(os.pardir, \"input\",\n",
    "                          \"jane-street-market-prediction-data\",\n",
    "                          \"stats.json\")\n",
    "\n",
    "with open(stats_file) as file:\n",
    "    stats = json.loads(file.read())\n",
    "\n",
    "# train the model\n",
    "hist = model.fit(train_ds,\n",
    "                 epochs=EPOCHS,\n",
    "                 steps_per_epoch=stats[str(HOLDOUT)][\"length\"] // BATCH_SIZE,\n",
    "                 validation_data=valid_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the learning curve and associated metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_df = pd.DataFrame(hist.history)\n",
    "\n",
    "# loss\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.lineplot(data=hist_df[\"loss\"], label=\"Training loss\")\n",
    "sns.lineplot(data=hist_df[\"val_loss\"], label=\"Validation loss\")\n",
    "# sns.lineplot(data=hist_df[\"lr\"], label=\"Learning rate\")\n",
    "plt.title(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"\")\n",
    "plt.show()\n",
    "\n",
    "# training metrics\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.lineplot(data=hist_df[\"precision\"], label=\"Precision\")\n",
    "sns.lineplot(data=hist_df[\"recall\"], label=\"Recall\")\n",
    "sns.lineplot(data=hist_df[\"auc\"], label=\"Area under ROC curve\")\n",
    "plt.title(\"Training metrics\")\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"\")\n",
    "plt.show()\n",
    "\n",
    "# validation metrics\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.lineplot(data=hist_df[\"val_precision\"], label=\"Precision\")\n",
    "sns.lineplot(data=hist_df[\"val_recall\"], label=\"Recall\")\n",
    "sns.lineplot(data=hist_df[\"val_auc\"], label=\"Area under ROC curve\")\n",
    "plt.title(\"Validation metrics\")\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model architecture and weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model.json\", \"w\") as f:\n",
    "    f.write(model.to_json())\n",
    "\n",
    "model.save_weights(\"model.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "papermill": {
   "duration": 5526.40816,
   "end_time": "2020-10-19T01:05:12.285603",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-10-18T23:33:05.877443",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
