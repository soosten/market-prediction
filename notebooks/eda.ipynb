{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About\n",
    "This notebook explores the dataset provided in the __[Jane Street Market Prediction](https://www.kaggle.com/c/jane-street-market-prediction)__ competition. The goal of the competition is to develop a model that will accept or reject trading opportunities based on \"real-time\" market data. The market data is presented in the form of a time series and at each point in time the model must accept/reject a new trade using only the data up to that time. At each point in time, we are given a vector\n",
    "\n",
    "` date, weight, feature_0, feature_1, ..., feature_129, ts_id`\n",
    "\n",
    "The times of the time series, `ts_id`, are grouped into days that are recorded in `date`. The features `feature_0, ...` consist of various market data has been \"anonymized\" (presumably to protect Jane Street's business model), so we don't really know what they represent. `weight` is the size of our investment in the proposed trade. The model should output 0 to reject the trade and 1 to accept it. Each trade results in a profit/loss response variable `resp`.\n",
    "\n",
    "For the $i$-th day, the model is given the score\n",
    "$$p_i = \\sum_j \\mbox{weight}_{ij} \\cdot \\mbox{resp}_{ij}$$\n",
    "where the sum ranges over all accepted trades for the day. The final score of the model over all days is then\n",
    "$$u = \\min(\\max(t, 0), 6) \\cdot \\sum_i p_i $$\n",
    "with\n",
    "$$t = \\frac{\\sum_i p_i}{\\sqrt{\\sum_i p_i^2}} \\sqrt{\\frac{250}{\\mbox{total number of days}}}.$$\n",
    "\n",
    "To train our model, we are given a file `train.csv` which contains 500 days worth of data or about 2 million data points (times) overall. For each point in time we are given the variables above along with the response variable `resp`. In addition, we are given 4 further response variables `resp_1, ..., resp_4`, which (according to the organizers) represent the profits achieved by the trade over different time scales. We are also given a boolean matrix in `features.csv`, which is explained below. Finally, we are given an example test set in `example_test.csv`, which contains some time series that are representative of what the model will see in production.\n",
    "\n",
    "# Summary\n",
    "This notebook consists of three sections. First, we examine correlations among the features. Then we train some simplistic models that ignore the time series structure and fit regressors for the response variable based only on the features at the same point in time. We attempt to analyze the predictive power of the most important features in these regression models. In the final section, we consider the time series structure of the data and try to determine the predictive power of historical data for the future.\n",
    "\n",
    "# Conclusions\n",
    "Most of the preliminary analysis indicates that we have a very noisy dataset that is prone to overfitting. It seems we should place an emphasis on a strong denoising / feature selection process and an ensemble of simple models on the new features. The score above emphasizes steady and consistent gains, so our model should have a low variance even if this costs some bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by running some setup code and parsing the CSV files provided in the dataset. We fill in missing values with the mean of that feature over all samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install datatable\n",
    "%pip install \"seaborn>=0.11.0\"\n",
    "\n",
    "import math\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import datatable as dt\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from statsmodels.tsa.stattools import acf, ccf, adfuller\n",
    "\n",
    "# make the labels legible on plots\n",
    "plt.rc('axes', labelsize=16)    # fontsize of the x and y labels\n",
    "\n",
    "comp_folder = os.path.join(os.pardir, \"input\", \"jane-street-market-prediction\")\n",
    "\n",
    "train_df = dt.fread(os.path.join(comp_folder, \"train.csv\")).to_pandas()\n",
    "train_df.set_index(\"ts_id\", inplace=True)\n",
    "test_df = pd.read_csv(os.path.join(comp_folder, \"example_test.csv\"), index_col=\"ts_id\")\n",
    "feat_df = pd.read_csv(os.path.join(comp_folder, \"features.csv\"), index_col=\"feature\")\n",
    "\n",
    "train_df.fillna(train_df.mean(), inplace=True)\n",
    "test_df.fillna(test_df.mean(), inplace=True)\n",
    "\n",
    "# list of columns with actual features\n",
    "feat_cols = [c for c in train_df.columns if \"feature\" in c]\n",
    "\n",
    "# keep a collection of any features which may be important\n",
    "# as we go through the notebook\n",
    "all_best_cols = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Correlations\n",
    "`features.csv` contains a boolean matrix whose $i,j$ entry is true if tag $i$ applies to feature $j$. A hypothetical example from the organizers is that the a following de-anonymized features and tags\n",
    "\n",
    "| feature | meaning  |\n",
    "|---:|:-------------|\n",
    "| feature_0 | volatility of this stock in past 30 days |\n",
    "| feature_1 | volume of this stock in past 30 days |\n",
    "| feature_2 | volume of this stock in past 10 days |\n",
    "\n",
    "| tag | meaning |\n",
    "|---:|:-------------|\n",
    "| tag_0 | some metric on the past 30 days  | False | \n",
    "| tag_1 | volume of this stock  | True  | \n",
    "\n",
    "would result in the following matrix:\n",
    "\n",
    "|  | tag_0  | tag_1 |\n",
    "|---:|:-------------|:-----------|\n",
    "| feature_0 | True  | False | \n",
    "| feature_1 | True  | True  | \n",
    "| feature_2 | False | True  |\n",
    "\n",
    "We plot the full matrix below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\"vmin\": 0, \"vmax\": 1,  \"linewidths\": .5, \"square\": True,\n",
    "           \"cbar\": False, \"cmap\": [\"white\", \"black\"], \"linecolor\": \"black\"\n",
    "          }\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "ax = sns.heatmap(feat_df.to_numpy().T, **options)\n",
    "\n",
    "ax.set_xlabel(\"Feature\")    \n",
    "ax.set_ylabel(\"Tag\")\n",
    "ax.set(xticklabels=[])\n",
    "ax.tick_params(bottom=False)\n",
    "ax.set(yticklabels=[])\n",
    "ax.tick_params(left=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check for correlations among the features. The image below is annotated and has a reasonable resolution in standalone viewers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = train_df[feat_cols].corr(method=\"pearson\")\n",
    "\n",
    "mask = np.triu(np.ones_like(corr, dtype=np.bool))\n",
    "options = {\"vmin\": -1, \"vmax\": 1,  \"linewidths\": .5, \"square\": True,\n",
    "           \"cbar\": False, \"cmap\": \"vlag\", \"mask\": mask, \"annot\": True,\n",
    "           \"linecolor\": \"black\"\n",
    "          }\n",
    "\n",
    "plt.figure(figsize=(80, 80))\n",
    "ax = sns.heatmap(corr.to_numpy(), **options)\n",
    "ax.set_xlabel(\"Feature\")\n",
    "ax.set_ylabel(\"Feature\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data set has 4 addtional response variables along with `resp` - the plots below generally show strong positive correlations among them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(train_df[[\"resp\", \"resp_1\", \"resp_2\", \"resp_3\", \"resp_4\"]], \n",
    "             diag_kind=\"kde\", plot_kws={\"s\": 3})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform a __[principal component analysis](https://en.wikipedia.org/wiki/Principal_component_analysis)__ and determine how much of the total variance is accounted for by each component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "pca = PCA()\n",
    "\n",
    "scaled = ss.fit_transform(train_df[feat_cols])\n",
    "pca.fit(scaled)\n",
    "\n",
    "var_perc = pd.DataFrame(100 * pca.explained_variance_ratio_, columns=[\"y\"])\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "ax = sns.barplot(x=var_perc.index, y=var_perc.y)\n",
    "ax.set_xlabel(\"Component\")    \n",
    "ax.set_ylabel(\"% of Total Variance\")\n",
    "ax.set(xticklabels=[])\n",
    "ax.tick_params(bottom=False)\n",
    "plt.show()\n",
    "\n",
    "rank = 1 + np.argmax(np.cumsum(pca.explained_variance_ratio_) >= 0.95)\n",
    "print(f\"{rank} components account for 95% of the total variance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Power\n",
    "To limit the computational power required, the remainder of this notebook only considers a subset of the data. More precisely, we restrict ourselves to the first time series on day 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneday = train_df.loc[train_df[\"date\"] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the __[mutual information](https://en.wikipedia.org/wiki/Mutual_information)__ of each feature with the response. Then we plot the three features with the highest mutual information against the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi = mutual_info_regression(oneday[feat_cols], oneday[\"resp\"], random_state=42)\n",
    "\n",
    "# 3 features with highest mutual information\n",
    "best = [f\"feature_{ix}\" for ix in np.argsort(-mi)[:3]]\n",
    "\n",
    "# add features to list of interesting features\n",
    "all_best_cols = all_best_cols + best\n",
    "\n",
    "mi_df = pd.DataFrame(mi, columns=[\"y\"])\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "ax = sns.barplot(x=mi_df.index, y=mi_df.y)\n",
    "ax.set_xlabel(\"Feature\")    \n",
    "ax.set_ylabel(\"Mutual Information with resp\")\n",
    "ax.set(xticklabels=[])\n",
    "ax.tick_params(bottom=False)\n",
    "plt.show()\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(20, 4))\n",
    "flag = False\n",
    "for (col, ax) in zip(best, axs):\n",
    "    sns.scatterplot(x=col, y=\"resp\", data=oneday, ax=ax, s=3)\n",
    "    if flag:\n",
    "        ax.set_ylabel(\"\")\n",
    "        ax.set(yticklabels=[])\n",
    "        ax.tick_params(left=False)\n",
    "    flag = True\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train a __[random forest](https://en.wikipedia.org/wiki/Random_forest)__ model and compute the __[importance](https://en.wikipedia.org/wiki/Random_forest#Properties)__ of each feature. Then we plot the three most important features against the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(max_features=\"auto\", random_state=42)\n",
    "rf.fit(oneday[feat_cols], oneday[\"resp\"])\n",
    "\n",
    "# 3 features with highest importance\n",
    "best = [f\"feature_{ix}\" for ix in np.argsort(-rf.feature_importances_)[:3]]\n",
    "\n",
    "# add features to list of interesting features\n",
    "all_best_cols = all_best_cols + best\n",
    "\n",
    "imp_df = pd.DataFrame(rf.feature_importances_, columns=[\"y\"])\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "ax = sns.barplot(x=imp_df.index, y=imp_df.y)\n",
    "ax.set_xlabel(\"Feature\")    \n",
    "ax.set_ylabel(\"Importance\")\n",
    "ax.set(xticklabels=[])\n",
    "ax.tick_params(bottom=False)\n",
    "plt.show()\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(20, 4))\n",
    "flag = False\n",
    "for (col, ax) in zip(best, axs):\n",
    "    sns.scatterplot(x=col, y=\"resp\", data=oneday, ax=ax, s=3)\n",
    "    if flag:\n",
    "        ax.set_ylabel(\"\")\n",
    "        ax.set(yticklabels=[])\n",
    "        ax.tick_params(left=False)\n",
    "    flag = True\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform a __[partial least squares](https://en.wikipedia.org/wiki/Partial_least_squares_regression)__ analysis and record the Variable Importance in Projection (VIP) scores for each feature. Then we plot the three most important features against the response. The VIP score implementation is taken from __[this](https://github.com/KevinMMendez/scikit-learn/commit/e532ac94dac52d6f7f1021970e887b83442f11e9)__ version of code presented in __[this](https://github.com/scikit-learn/scikit-learn/issues/7050)__ issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "pls = PLSRegression(n_components=len(feat_cols))\n",
    "\n",
    "X = ss.fit_transform(oneday[feat_cols])\n",
    "y = oneday[\"resp\"]\n",
    "pls.fit(X, y)\n",
    "\n",
    "# compute VIP scores\n",
    "T = pls.x_scores_\n",
    "W = pls.x_weights_\n",
    "Q = pls.y_loadings_\n",
    "w0, w1 = W.shape\n",
    "s = np.sum(T ** 2, axis=0) * np.sum(Q ** 2, axis=0)\n",
    "s_sum = np.sum(s, axis=0)\n",
    "w_norm = np.array([(W[:, i] / np.linalg.norm(W[:, i])) for i in range(w1)])\n",
    "vip = np.sqrt(w0 * np.sum(s * w_norm.T ** 2, axis=1) / s_sum)\n",
    "\n",
    "# 3 features with highest VIP\n",
    "best = [f\"feature_{ix}\" for ix in np.argsort(-vip)[:3]]\n",
    "\n",
    "# add features to list of interesting features\n",
    "all_best_cols = all_best_cols + best\n",
    "\n",
    "vip_df = pd.DataFrame(vip, columns=[\"y\"])\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "ax = sns.barplot(x=vip_df.index, y=vip_df.y)\n",
    "ax.set_xlabel(\"Feature\")    \n",
    "ax.set_ylabel(\"Variable Importance in Projection\")\n",
    "ax.set(xticklabels=[])\n",
    "ax.tick_params(bottom=False)\n",
    "plt.show()\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(20, 4))\n",
    "flag = False\n",
    "for (col, ax) in zip(best, axs):\n",
    "    sns.scatterplot(x=col, y=\"resp\", data=oneday, ax=ax, s=3)\n",
    "    if flag:\n",
    "        ax.set_ylabel(\"\")\n",
    "        ax.set(yticklabels=[])\n",
    "        ax.tick_params(left=False)\n",
    "    flag = True\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Analysis\n",
    "The dataset has a temporal structure. For each value of the `date` column, we have a sequence of samples indexed by a \"time\" `ts_id`. Not all of the time series have the same length, but all of them have at least 10000 entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "ax = sns.histplot(data=train_df, x=\"date\", edgecolor=\"black\")\n",
    "\n",
    "ax.set_xlabel(\"Date\")    \n",
    "ax.set_ylabel(\"Number of Timestamps\")\n",
    "ax.set(xticklabels=[])\n",
    "ax.tick_params(bottom=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __[augmented Dickey-Fuller test](https://en.wikipedia.org/wiki/Augmented_Dickey%E2%80%93Fuller_test)__ rejects the hypothesis of a unit root and the response looks like a stationary process. The __[autocorrelation function](https://en.wikipedia.org/wiki/Autocorrelation)__ decays rapidly and high frequencies are well-represented in the Fourier spectrum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf = adfuller(oneday[\"resp\"])   \n",
    "print(f\"ADF statistic: {adf[0]} \\t p-value: {adf[1]}\")\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "sns.lineplot(data=oneday[\"resp\"], ax=axs[0])\n",
    "axs[0].set_xlabel(\"Time\")    \n",
    "axs[0].set_ylabel(\"resp\")\n",
    "\n",
    "auto_cor = acf(oneday[\"resp\"], fft=True)\n",
    "sns.lineplot(data=auto_cor, ax=axs[1])\n",
    "axs[1].set_xlabel(\"Lag\")    \n",
    "axs[1].set_ylabel(\"Autocorrelation\")\n",
    "\n",
    "ft_abs = np.abs(np.fft.rfft(oneday[\"resp\"]))\n",
    "sns.lineplot(data=ft_abs, ax=axs[2])\n",
    "axs[2].set_xlabel(\"Frequency\")    \n",
    "axs[2].set_ylabel(\"Amplitude\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the __[cross-correlation functions](https://en.wikipedia.org/wiki/Cross-correlation)__ between the significant features identified in the previous section and the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_best_cols = list(set(all_best_cols))\n",
    "\n",
    "nrows = math.ceil(len(all_best_cols) / 3)\n",
    "fig, axs = plt.subplots(nrows, 3, figsize=(20, 5 * nrows))\n",
    "\n",
    "for (col, ax) in zip(all_best_cols, axs.flatten()):\n",
    "    cross_cor = ccf(oneday[col], oneday[\"resp\"])[:40]\n",
    "    sns.lineplot(data=cross_cor, ax=ax)             \n",
    "    ax.set_title(col)\n",
    "\n",
    "if nrows > 1:\n",
    "    for ax in axs[:, 0]:\n",
    "        ax.set_ylabel(\"Cross-correlation\")\n",
    "\n",
    "    for ax in axs[nrows - 1, :]:\n",
    "        ax.set_xlabel(\"Lag\")\n",
    "\n",
    "else:\n",
    "    axs[0].set_ylabel(\"Cross-correlation\")\n",
    "    for ax in axs:\n",
    "        ax.set_xlabel(\"Lag\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are there correlations between the days? (unlikely, given the decay of correlations)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
